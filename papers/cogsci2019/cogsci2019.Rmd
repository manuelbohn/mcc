---
title: "Integrating Common Ground and Informativeness in Pragmatic Word Learning"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
   \author{{\large \bf Manuel Bohn} \\ \texttt{bohn@stanford.edu} \\ Psychology, Stanford University \\ LFE, Leipzig University 
    \And {\large \bf Michael Henry Tessler} \\ \texttt{tessler@mit.edu} \\ Department of Brain and Cognitive Sciences \\ Massachusetts Institute of Technology
    \And {\large \bf Michael C. Frank} \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University}

abstract: >
    Pragmatic inferences are an integral part of language learning and comprehension. To recover the intended meaning of an utterance, listeners need to balance and integrate different sources of contextual information. In a series of experiments, we studied how listeners integrate general expectations about speakers with expectations specific to their interactional history with a particular speaker. We used a Bayesian pragmatics model to formalize the integration process. In Experiments 1 and 2, we replicated previous findings showing that listeners make inferences based on speaker-general and speaker-specific expectations. We then used the empirical measurements from these experiments to generate model predictions about how the two kinds of expectations should be integrated, which we tested in Experiment 3. Experiment 4 replicated and extended Experiment 3 to a broader set of conditions. In both experiments, listeners based their inferences on both types of expectations. We found that model performance was also consistent with this finding; with better fit for a model which incorporated both general and specific information compared to baselines incorporating only one type. Listeners flexibly integrate different forms of social expectations across a range of contexts, a process which can be described using Bayesian models of pragmatic reasoning.
    
keywords: >
    Pragmatics; Word learning; Common ground; Bayesian models
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(knitr)
library(ggthemes)
library(langcog)
library(rwebppl)
library(matrixStats)
library(coda)
library(ggpubr)
library(lme4)
library(broom)
library(exactRankTests)
```

```{r}
# data ex 1 and ex 2
d1 <- bind_rows(read_csv(file="../../stats/data/novel.data.csv"),
               read_csv(file="../../stats/data/pref.data.csv")) %>%
  mutate(trial_type = ifelse(trial == "train", "train", "test"),
         control = ifelse(change == "true", "true" , "false"))

d2 <- read_csv(file="../../stats/data/inf.data.csv") %>%
  mutate(trial_type = ifelse(trial == "train", "train", "test"))

dex12 <- bind_rows(d1,d2) %>%
  filter(trial_type == "test")%>%
  mutate(control= relevel(factor(control), ref = "true"),
         choiceAgent = ifelse( control == "false", agent, altAgent)) # re-code agent for preference and novelty 

# data ex 3
d3 <- bind_rows(read_csv(file="../../stats/data/ex3.novel.data.csv"),
               read_csv(file="../../stats/data/ex3.pref.data.csv")) %>%
  mutate(trial_type = ifelse(trial == "train1" | trial =="train2", "train", "test"))

# check if someone did both experiments
# yes, two people
x <- d3 %>%
  filter(trial_type == "train")%>%
  group_by(id)%>%
  summarise(n = length(correct_inf)) %>%
  filter(n > 2)

# exclude those who did both experiments
dex3 <- d3 %>%
  filter(!id %in% x$id) %>%
  mutate(Speaker = ifelse(change =="same", "Same speaker", "Different speaker"),
         Alignment = ifelse(alignment == "congruent", "Congruent", "Incongruent"),
         Prior = ifelse(experiment == "pref_inf", "Preference", "Novelty"),
         choiceAgent = ifelse (change == "same", agent, altAgent))%>%
  filter(trial_type == "test")



d4 <- bind_rows(read_csv(file="../../stats/data/ex3.2.novel.strong.data.csv"),
               read_csv(file="../../stats/data/ex3.2.novel.medium.data.csv"),
               read_csv(file="../../stats/data/ex3.2.novel.weak.data.csv"),
               read_csv(file="../../stats/data/ex3.2.pref.strong.data.csv"),
               read_csv(file="../../stats/data/ex3.2.pref.medium.data.csv")
               ) %>%
  mutate(trial_type = ifelse(trial == "train1" | trial =="train2", "train", "test"))


x <- d4 %>%
  filter(trial_type == "train") %>%
  group_by(id)%>%
  summarise(correct_inf = mean(correct_inf)) %>%
  filter(correct_inf == 0)


dex4 <- d4 %>%
  filter(!id %in% x$id)%>% 
  distinct(id, alltrial, .keep_all = TRUE) %>%
  filter(trial_type != "train") %>%
  mutate(Change = ifelse(change =="same", "Same speaker", "Different speaker"),
         Alignment = ifelse(alignment == "congruent", "Congruent", "Incongruent"),
         Experiment = ifelse(grepl("pref",experiment), "Preference", "Novelty"),
         Prior = ifelse(prior == "weak", "Weak", ifelse(prior == "medium", "Medium", "Strong"))
         )


# loading model comparisons and predictions

pred_ex3 <- readRDS(file = "../../stats/saves/model_predictions_ex3_1.rds")

pred_ex4 <- readRDS(file = "../../stats/saves/model_predictions_ex3_variable_priors.rds")

bf_ex3 <- readRDS(file = "../../stats/saves/ex3_bf.rds")

bf_ex4 <- readRDS(file = "../../stats/saves/ex4_bf.rds")

```

# Introduction 

One of the most astonishing features of natural language is that it allows us to communicate precise meanings despite the fact that most utterances are inherently ambiguous. While the conventional mapping between sounds (words) and objects constrain what a speaker may mean by an utterance, the intended meaning of the utterance is not reducible to the words that are contained in it. It takes additional pragmatic inference to recover the intended meaning [@levinson2000presumptive].

Pragmatic inferences rest on a set of expectations that interlocutors bring to the table when entering a communicative interaction. On the one hand, speakers and listeners have the general expectation that their partner communicates in an informative and relevant way [@sperber2001relevance]. Grice [-@grice1991studies] summarised this expectation via the *Cooperative Principle*: “Make your contribution such as is required, at the stage at which it occurs, by the accepted purpose or direction of the talk exchange in which you are engaged.” Importantly, the second half of the Cooperative Principle describes a second type of expectation: interlocutors expect each other to produce and interpret utterances *in light of the shared common ground between them* [@clark1996using]. Common ground refers to bits of information that are assumed to be shared, either because they were mentioned over the course of the conversation or grounded through some form of joint experience [@bohn2018common]. Note that by its very nature, common ground may vary with the individuals involved in a particular conversation. 

These same general and specific expectations can support children’s word learning [@clark2009first; @tomasello2009constructing]. On the one hand, children have been found to learn novel words by assuming speakers are generally informative [@frank2014inferring]. That is, in the absence of any prior interaction with the speaker, children interpreted a novel word as referring to the most informative referent. On the other hand, children use conversation-specific common ground expectations to decide which object a specific speaker is referring to when they use a novel word [@akhtar1996role]. For example, when a speaker expressed preference for a particular object, children expect a novel word from the same speaker to refer to the previously preferred object [@saylor2009preschoolers].

But how do listeners integrate general and common ground-related expectations during word learning? Are pragmatic inferences strengthened additively when both support a particular interpretation? How are they weighed when they are in conflict? The Rational Speech Act (RSA) framework [@frank2012predicting; @goodman2016pragmatic] offers a formal framework for addressing this information integration problem. RSA models are characterized by a recursive structure in which a pragmatic listener tries to uncover a speaker’s intended meaning by assuming the speaker chose their utterance in order to get a naive listener to recover their intended meaning. RSA models have made accurate quantitative predictions about various forms of pragmatic language use and word learning [@goodman2016pragmatic]. However, a comprehensive treatment of how general and common ground expectations are integrated is still missing.

Within RSA models, each agent in this recursion is modeled as a Bayesian reasoner; thus, information integration is treated as a process of probabilistic inference. The speaker-general informativeness expectation is already encoded in the structure of the model: Speakers produce utterances to aid the listener in disambiguating referents. We operationalize speaker-specific, common ground information as the shared prior probability of referents in the context of the utterance. Thus, a natural locus for information integration within these models is the trade off between the prior probability of a particular referent and the likelihood of that referent given the current utterance. 

Here we evaluate this rational, pragmatic account of information integration. We isolate speaker-specific and common-ground information experimentally, then test how adult listeners combine them in a word learning setting. In Experiments 1 and 2, we replicate findings showing that listeners expect speakers to a) produce informative utterances (Experiment 1) and b) communicate about things that are relevant to common ground (Experiment 2). Based on these results, we generate model predictions using the RSA framework about how these two components should be integrated. In Experiment 3, we test how listeners integrate their expectations and compare model predictions to empirical data. Experiment 4 replicates and extends Experiment 3 by varying the strength of common ground assumptions.

# Method

## General Design

Experiments were conducted online using Amazon’s Mechanical Turk. Fig. \ref{fig:design} provides a schematic overview of the setup and experimental procedures. The instructions informed participants that they would see a number of animal characters asking for novel toys. Participants were asked to identify the toy being requested by a particular animal. The basic layout involved two tables with toys on them, located left and right of a little hill, on which the animal was standing. For each animal, we recorded a set of utterances (one speaker per animal) that were used throughout the experiments to provide information and make requests. At test, toys were requested using the following utterance: “Oh cool, there is a [non-word] on the table, how neat, can you give me the [non-word]?”. Participants responded by clicking on one of the toys. Each experiment started with two training trials in which animals requested familiar objects (car and ball).

For all experiments, we pre-registered the sample size, experimental design and the statistical analysis. For Experiment 3 and 4, we also registered the model structure and predictions (see [masked for peer review])

```{r design, fig.env = "figure*", fig.pos = "h", fig.width=6, fig.height=5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Schematic experimental procedure. In all conditions, at test (bottom), the speaker ambiguously requested an object using a non-word (e.g. “dax”). Participants clicked on the object they thought the speaker referred to. Informativeness (Experiment 1, left) translated to making one object less frequent in context. Common ground (Experiment 2, middle) was manipulated by making one object prefered by or new to the speaker. Green plus signs represent an expression of preference and red minus of dispreference (see main text for details). When expressing e.g. preference for an object on a table with two objects (panel 3 from top), the respective object was temporarily enlarged. Experiment 3 (right) combined manipulations. Condition shown here: preference - same speaker - incongruent."}
img <- png::readPNG("figs/method_figure.png")
grid::grid.raster(img)
```

# Experiment 1

## Participants, Design and Procedure

All participants were recruited from Amazon Mechanical Turk and had US IP addresses. Sample size in each experiment was planned to be 120 data points per cell. Experiment 1 had 40 participants. In the test condition, one table contained one object of type A and the other table contained one object of type A and one of type B (see Fig. \ref{fig:design}, left). On each trial, the animal introduced themselves (e.g. “Hi, I’m Dog”), turned towards the table with the two objects and made a request. If listeners expect speakers to produce informative utterances, they should select object B. This choice follows from the counterfactual inference that if the (informative) speaker would have wanted to request A, they would have turned to the table that only contained A. On the other hand, since B is only located on the table together with A, there was no alternative way to request B in a less ambiguous way. In the control condition, both tables contained two objects, one of which was randomly determined as the correct one. No inference was therefore licensed. Each participant received three trials in each condition for a total of six trials, presented in a randomized order.

## Results and Discussion

```{r ex1-results}
ex1t <- dex12 %>%
  filter(condition == "informativeness")%>%
  group_by(control, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(control) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         p_value = t.test(unlist(correct), mu = 0.5)$p.value) %>%
  select(control,df,t_value,p_value)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2))


# model
lm_ex1 <- glmer(correct ~ control +
              (control |id) + (control |agent), 
              data = dex12%>%filter(condition == "informativeness"), 
              family = binomial,control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))


ex1_r <- tidy(lm_ex1, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))
```

Participants selected the less frequent object above chance in the test condition (t(`r ex1t%>%filter(control == "false")%>%pull(df)`) = `r ex1t%>%filter(control == "false")%>%pull(t_value)`, *p* `r ex1t%>%filter(control == "false")%>%pull(p_value)`, see Fig. \ref{fig:plotexp12}) and did so more often compared to the control condition (generalized linear mixed model (GLMM\footnote{All models included random effects for subject and speaker.}):*$\beta$* = `r ex1_r%>%filter(term == "controlfalse")%>%pull(estimate)`, se = `r ex1_r%>%filter(term == "controlfalse")%>%pull(std.error)` *p* `r ex1_r%>%filter(term == "controlfalse")%>%pull(p.value)`). This result replicates earlier work [@frank2014inferring] and is consistent with the hypothesis that listeners expect speakers to communicate in an informative way.

```{r plotexp12, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=3.4, fig.height=2, fig.cap = "Results from Experiment 1 and 2. For preference and novelty, control refers to a different speaker (see Fig. 1). Transparent dots show data from individual participants, solid dots represent condition means, error bars are 95\\% CIs. Dashed line indicates performance expected by chance."}

p1 <- dex12 %>%
  filter(trial_type == "test") %>%
  mutate(control = ifelse(control == "true", "Control", "Test"),
         condition = factor(condition), 
         experiment = ifelse(condition == "informativeness", "1","2"))%>%
  group_by(experiment,condition ,control, id) %>%
  summarise(correct = mean(correct))

p2 <- p1 %>%
  multi_boot_standard(col = "correct")


 ggplot() +
  geom_jitter(data = p1, aes(x = control, y = correct, col = control), alpha = 0.2, width = .3,height = .025)+
  geom_pointrange(data = p2, aes(x = control, y = mean, col = control,ymin = ci_lower, ymax = ci_upper),size = .5)+
  geom_hline(yintercept = 0.5, lty=2)+
  labs(x="",y="Expected Choice")+
  facet_wrap( ~ condition, labeller = as_labeller(c(`preference`="Exp. 2 - Pref.", `novelty`="Exp. 2 - Nov.", `informativeness`="Exp. 1 - Inf.")))+
  theme_few(base_size = 10) + 
  ylim(-0.05,1.05)+
  guides(alpha = F,size =F, col = F)+
  scale_color_manual(name = "Condition",
                     breaks=c("Test","Control"),
                     values= c("#839496","black"))
 
```

# Experiment 2

We manipulated common ground expectations based on procedures that have successfully been used in developmental studies [e.g. @akhtar1996role; @saylor2009preschoolers]. Speakers either expressed preference for one object or one object was new to the speaker.

## Participants, Design and Procedure

We collected data from 80 participants, with 40 in each condition. In the preference condition, each table had a different object. In the beginning, the animal appeared on the hill and introduced themselves. Next, they turned to one of the tables and expressed either that they liked (“Oh wow, I really like that one”) or disliked (“Oh bleh, I really don’t like that one”) the object before turning to the other side and expressing the respective other attitude. Then, the animal disappeared. After a short period of time, either the same or a different animal appeared and requested an object while facing straight ahead (see Fig. \ref{fig:design}, middle). If participants took into account the information they gained about the speaker, they should select the previously preferred object if the returning animal was the same. If a different animal returned, they should choose randomly between objects.

In the novelty condition, one table was initially empty while there was an object on the other table (see Fig. \ref{fig:design}). The animal turned to one of the sides and commented either on the presence (“Aha, look at that”) or the absence of an object (“Hm, nothing there”). Next, the animal disappeared. The same animal re-appeared and the sequence above was repeated. When the animal disappeared for the second time, a second object appeared on the empty table while the animal was away. Like in the preference condition, we now manipulated if the same animal or a different one returned. In case of the same animal returning, listeners could infer the referent of the subsequent request by considering that one object was new to the speaker and therefore more likely to be of interest to them. However, no such inference was licensed when a different animal returned because both objects were novel.

## Results and Discussion

```{r ex2-results-pref}
ex2tpref <- dex12 %>%
  filter(condition == "preference")%>%
  group_by(control, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(control) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         p_value = t.test(unlist(correct), mu = 0.5)$p.value) %>%
  select(control,df,t_value,p_value)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2))


# model (maximally converging)
lm_ex2pref <- glmer(correct ~ control +
              (1|id) + (control |choiceAgent), 
              data = dex12%>%filter(condition == "preference"), 
              family = binomial,control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))


ex2pref_r <- tidy(lm_ex2pref, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))
```

```{r ex2-results-nov}
ex2tnov <- dex12 %>%
  filter(condition == "novelty")%>%
  group_by(control, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(control) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         p_value = t.test(unlist(correct), mu = 0.5)$p.value) %>%
  select(control,df,t_value,p_value)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2))


# model
lm_ex2nov <- glmer(correct ~ control +
              (1|id) + (control |choiceAgent), 
              data = dex12%>%filter(condition == "novelty"), 
              family = binomial,control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))


ex2nov_r <- tidy(lm_ex2nov, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))
```

Participants selected the preferred object above chance when the same animal returned (t(`r ex2tpref%>%filter(control == "false")%>%pull(df)`) = `r ex2tpref%>%filter(control == "false")%>%pull(t_value)`, *p* `r ex2tpref%>%filter(control == "false")%>%pull(p_value)`, see Fig. \ref{fig:plotexp12}) and did so more often compared to trials in which a different animal returned (GLMM: $\beta$ = `r ex2pref_r%>%filter(term == "controlfalse")%>%pull(estimate)`, se = `r ex2pref_r%>%filter(term == "controlfalse")%>%pull(std.error)`, *p* `r ex2pref_r%>%filter(term == "controlfalse")%>%pull(p.value)`). Thus, listeners inferred the referent of the utterance by considering previous interactions with the speaker. Interestingly, participants transferred preference to some extent from one animal to the other and selected the preferred object above chance when a different animal returned (t(`r ex2tpref%>%filter(control == "true")%>%pull(df)`) = `r ex2tpref%>%filter(control == "true")%>%pull(t_value)`, *p* `r ex2tpref%>%filter(control == "true")%>%pull(p_value)`). In sum, this study shows that adults make comparable inferences to children [@saylor2009preschoolers].

The novel object was selected above chance when the same animal returned (t(`r ex2tnov%>%filter(control == "false")%>%pull(df)`) = `r ex2tnov%>%filter(control == "false")%>%pull(t_value)`, *p* `r ex2tnov%>%filter(control == "false")%>%pull(p_value)`) but not when a different one appeared (t(`r ex2tnov%>%filter(control == "true")%>%pull(df)`) = `r ex2tnov%>%filter(control == "true")%>%pull(t_value)`, *p* `r ex2tnov%>%filter(control == "true")%>%pull(p_value)`, see Fig. \ref{fig:plotexp12}). Furthermore, the two conditions differed in the expected direction ($\beta$ = `r ex2nov_r%>%filter(term == "controlfalse")%>%pull(estimate)`, se = `r ex2nov_r%>%filter(term == "controlfalse")%>%pull(std.error)`, *p* `r ex2nov_r%>%filter(term == "controlfalse")%>%pull(p.value)`). Thus, like children [@akhtar1996role], adults used their prior information about the speaker to resolve ambiguity in the utterance.

# Experiment 3

```{r plotexp3, fig.env="figure*", fig.pos = "h", fig.width=7, fig.height=2.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Results from Experiment 3. Dashed line indicates performance expected by chance. Plotting conventions are the same as in Fig. 2. All conditions in which CIs do not overlap with chance line are also statistically different from chance based on two-tailed Wilcoxon tests."}

p3 <- dex3 %>%
  group_by(Prior,Speaker ,Alignment, id) %>%
  summarise(correct = mean(correct_inf))

p4 <- p3 %>%
  multi_boot_standard(col = "correct")


ggplot() +
  #geom_rect(data = p4, aes(fill = NA), color = "#859900", size = 2.5, alpha = 0.4 ,xmin = -Inf,xmax = Inf, ymin = -Inf,ymax = Inf)+
  geom_jitter(data = p3, aes(x = Alignment, y = correct), alpha = 0.2, width = .3,height = .025)+
  geom_pointrange(data = p4, aes(x = Alignment, y = mean,ymin = ci_lower, ymax = ci_upper),size = .5)+
  geom_hline(yintercept = 0.5, lty=2)+
  labs(x="",y="Choosing More Informative")+
  facet_grid ( ~ Prior + Speaker) +
  theme_few(base_size = 10) + 
  ylim(-0.05,1.05)+
  guides(alpha = F, fill = F, col = F)+ 
  scale_fill_manual(name="Condition",
                     breaks=c("1","2","3"),
                     values= c("#859900","#859900","#859900"))
 
```

In Experiment 3, we combined the expectations studied in Experiment 1 and 2 to see how listeners integrate them. 

## Participants, Design and Procedure

A total of 121 individuals participated in the experiment. The test situation was the same as in the test condition in Experiment 1: One table with object of type A and the other with an object of type A and B. Again, the animal always turned to the table with two objects and ambiguously requested an object. In Experiment 1, the listener had no prior information about each object. In Experiment 3, however, we manipulated common ground expectations in the same way as in Experiment 2. For example, the animal would turn to the table with one object and express that they don’t like object A, then turn to the other table and express that they like object B. Next, after quickly disappearing, they would reappear, turn to the table with two objects and make a request (see Fig. \ref{fig:design}, right).

For each common ground condition, there were 4 conditions in Experiment 3 resulting from the cross of congruent/incongruent informativeness with same/different speaker. If the preferred/novel object was the less frequent one (object B), the two expectations were congruent. If the preferred/novel object was the more frequent one (object A), expectations were incongruent. For each type of expectation alignment, we varied if the same or a different animal returned. Participants either completed the preference or novelty version with two test trials in each of the four conditions. Before discussing the empirical results, we briefly discuss the cognitive model we used to predict expectation integration.

## Model Predictions

To derive predictions, we used a probabilistic RSA model that simulates a pragmatic listener reasoning about a cooperative speaker who is trying to refer to an object [@frank2012predicting]. The speaker chooses how to refer to the object by reasoning about a naive listener who does not know the labels for the object [@frank2014inferring]. The conditional probability that the listener chooses a referent given an utterance is defined as follows:
$$P_L(r_s|u)\propto P_S(u|r_s)P_S(r_s)$$
Here, $P_S(u|r_s)$ is the likelihood that the speaker will use an utterance u to refer to a specific referent r. It is defined in terms of a utility function $U_S(u;s)$ consisting of the surprisal of $u$ for a naive listener $L_0$, who interprets $u$ according its literal semantics:
$$P_S(u|r_s)\propto exp(\alpha U_S(u;s))$$
The numerical strength of the expression above depends on a scalar value, $\alpha$, which can be interpreted as an indicator of how rational the speaker is in choosing utterances (i.e. as $\alpha$ increases, the speaker is more likely to choose the most informative utterance).

The term $P_S(r_s)$ denotes the prior probability that a speaker will refer to a given referent. This probability represents the listeners expectations about the speaker depending on the manipulation (preference or novelty) and the identity of the speaker (same or different speaker).

We used the results from Experiment 1 and 2 to specify $\alpha$ as well as $P_S(r_s)$ in our model. We set $\alpha$ so that a model with uniform priors would predict the average proportion measured in Experiment 1. The prior probability for each object was set to be the proportion with which this object was chosen in Experiment 2\footnote{Proportions were measured when participants chose between two objects. However, in Experiment 3, three objects were involved. For each object we used the proportion measured in Experiment 2 as the prior probability.This approached spread out the absolute probability mass for each object but conserved the relative relation between objects.}. Based on these parameter settings, we predicted the proportion with which listeners will choose the more informative object in each of eight unique conditions mentioned above (see also Fig. \ref{fig:plotexp3}). We compared the fit of this pragmatic model to two alternative models using Bayes Factors (BF). The first alternative model ignored the speaker specific expectations (uniform prior model) while the second ignored the informativeness inference (prior only model). All models included a noise parameter, reflecting that participants may respond randomly instead of in line with the intended manipulation on a given trial. Noise parameters were estimated based on the data. They range between 0 and 1 and reflect the proportion of responses that are estimated to be random instead of following the pattern predicted by the model.

<!-- All models included a “noise parameter”, reflecting that participants may respond randomly instead of in line with the intended manipulation on a given trial. Noise parameters were estimated based on the data. -->

## Results and Discussion

```{r ex3_tab, results="asis"}
#comaprison to chance
ex3_tab <- dex3 %>%  
  filter(trial_type == "test") %>%
  group_by(Speaker ,Prior,Alignment, id) %>%
  summarise(correct_inf = mean(correct_inf)) %>%
  summarize(correct_inf = list(correct_inf)) %>%
  group_by(Speaker ,Prior,Alignment) %>%
  mutate(M = mean(unlist(correct_inf)),
         W = wilcox.exact(unlist(correct_inf), mu = 0.5)$statistic,
         p = wilcox.exact(unlist(correct_inf), mu = 0.5)$p.value) %>%
  select(Prior,Alignment, Speaker, M , W,p)%>%
  mutate(M = round(M,2),
         p = ifelse(p<.001,"< .001",as.character(substr(round(p,3),2,5),sep=" ")))

#tab1 <- xtable::xtable(ex3_tab, caption = "Proportion of choice of less frequent object for each unique condition in Experiment 3.", label = "bubub")

#print(tab1, type="latex", comment = F, table.placement = "H")

```

```{r ex3_results, cache=T}
# model 
lm_ex3 <- glmer(correct_inf ~ Prior*Speaker*Alignment + (Speaker+Alignment|id) + (Speaker+Alignment|choiceAgent), 
              data = dex3, family = binomial, 
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

#summary(lm_ex3)

lm_ex3_r <- tidy(lm_ex3, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))

```

```{r model_comp}

model_comp <- cor.test(
  pred_ex3%>%filter(Model == "Noisy RSA Model")%>%pull(mean),
  pred_ex3%>%filter(Model == "Noisy RSA Model")%>%pull(Data)
  )

model_comp_r <- tidy(model_comp) %>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         estimate = round(estimate,2))
  
```

Results are discussed in the form of the proportion with which listeners chose the more informative object (i.e., the object that would be the more informative referent when only considering speaker general information). For a comparison to chance within each unique condition see \ref{fig:plotexp3}. Combinations of alignment and speaker identity differed in how they influenced participants’ responses (GLMM model term: `alignment*speaker`; $\beta$ = `r lm_ex3_r%>%filter(term == "SpeakerSame speaker:AlignmentIncongruent")%>%pull(estimate)`, se = `r lm_ex3_r%>%filter(term == "SpeakerSame speaker:AlignmentIncongruent")%>%pull(std.error)`, *p* `r lm_ex3_r%>%filter(term == "SpeakerSame speaker:AlignmentIncongruent")%>%pull(p.value)`). Fig. \ref{fig:plotmodelcomp}A and B shows the mean response in each unique condition compared to the pragmatics model. Model predictions and data were highly correlated (r = `r model_comp_r$estimate`, *p* `r model_comp_r$p.value`).Model fit was much better in the model taking into account both types of expectations compared to the uniform prior (BF = `r format(bf_ex3%>%filter(Comparison == "Pragmatic vs. No prior") %>%pull(BayesFactor),digits = 2)`) or prior only model (BF = `r format(bf_ex3%>%filter(Comparison == "Pragmatic vs. Prior only") %>%pull(BayesFactor), digits = 2)`). The inferred noise level in the pragmatics model was 0.27 (95% HDI: 0.21 - 0.34].

Interestingly, as in Experiment 2, there was a transfer of preference in the case of speaker change. Participants were at chance in the preference - different speaker - incongruent condition (see Fig. \ref{fig:plotexp3}). If preference would have been specific to a particular individual, participants should have selected the less frequent object above chance (as they did in the corresponding condition with the novelty manipulation). Because it takes into account the measurement from the earlier experiment, our model predicts these results; future work might explicitly model generalization across speakers.

# Experiment 4

Here we replicated and extended Experiment 3 by manipulating the strength of the common ground expectations.

## Participants, Design and Procedure

This experiment had 453 participants. The structure of the experiment was the same as in Experiment 3. For each common ground expectation (preference and novelty), we intended to have a strong, a medium and a weak condition. The strength of each condition was determined by the proportion with which participants chose the preferred/novel object given the manipulation. We succeeded in generating quantitative variability for novelty. For preference we piloted a number of additional manipulations but did no find one that yielded a weaker preference compared to a medium condition. 

The strong manipulations were identical to Experiment 3 and the results are therefore a direct replication (see Fig. \ref{fig:plotmodelcomp}C). For novelty, in the medium condition, the animal turned to each table only once before the test. In the weak condition, the animal only turned to the table with an object before the test (instead of turning to and commenting on both). In the medium condition for preference, the animal only expressed liking and did so in a more subtle way (saying only: “Oh, wow” while pointing to the object). Participants were assigned to one level of common ground expectation and completed two test trials in each of the four conditions (alignment x speaker change).

Model predictions were obtained in the same way as in Experiment 3; with $\alpha$ inferred from the data and $P_S(r_s)$ measured empirically (in a set of corresponding experiments parallel to Experiment 2).

## Results and Discussion

```{r plotmodelcomp, fig.env="figure*", fig.pos = "h", fig.width=7, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "(A) Model predictions compared to data for Experiment 3 and (B) Experiment 4. (C) Data for strong prior manipulation in Experiment 3 and 4, providing a noise ceiling for the reliability of measurements. Error bars = 95\\% HDIs."}

pc1 <- pred_ex3%>%
  filter(Model == "Noisy RSA Model")%>%
  mutate(Prior = "Strong")
  
c1 <-ggplot(data = pc1,aes(x = mean, y = Data, col = Prior)) +
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 1, size = .5)+
  geom_point(size = 2)+
  geom_errorbar(aes(ymin = Data_ci_low, ymax = Data_ci_up),width = 0,size = .5)+
  geom_errorbarh(aes(xmin = ci_lower, xmax = ci_upper), height = 0,size = .5)+
  stat_cor(method = "pearson", label.x = 0.01, label.y = 0.99, aes(x = mean, y = Data), inherit.aes = F, size = 3)+
  coord_fixed()+
  xlim(0,1)+ylim(0,1)+
  xlab("Model")+
  theme_few(base_size = 10) + 
  guides(col = F)+
  scale_colour_viridis_d()

pc2 <- pred_ex4%>%
  filter(Model == "Noisy RSA Model")

c2 <- ggplot(data = pc2,aes(x = mean, y = Data, col = Prior)) +
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 1, size = .5)+
  geom_point(size = 2)+
  geom_errorbar(aes(ymin = Data_ci_low, ymax = Data_ci_up),width = 0,size = .5 )+
  geom_errorbarh(aes(xmin = ci_lower, xmax = ci_upper), height = 0,size = .5)+
  stat_cor(method = "pearson", label.x = 0.01, label.y = 0.99, aes(x = mean, y = Data), inherit.aes = F, size = 3)+
  coord_fixed()+
  xlim(0,1)+ylim(0,1)+
  xlab("Model")+
  theme_few(base_size = 10) + 
  scale_colour_manual(values = rev(viridis::viridis(3)))

pc3 <- data.frame(
    ex3 = pc1%>%pull(Data),
    ex3_ci_l = pc1%>%pull(Data_ci_low),
    ex3_ci_u = pc1%>%pull(Data_ci_up),
    ex4 = pc2%>%filter(Prior == "Strong")%>%pull(Data),
    ex4_ci_l = pc2%>%filter(Prior == "Strong")%>%pull(Data_ci_low),
    ex4_ci_u = pc2%>%filter(Prior == "Strong")%>%pull(Data_ci_up),
    Prior = "Strong")

c3 <- ggplot(data = pc3,aes(x = ex3, y = ex4, col = Prior)) +
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 1, size = .5)+
  geom_point(size = 2)+
  geom_errorbar(aes(ymin = ex4_ci_l, ymax = ex4_ci_u),width = 0,size = .5 )+
  geom_errorbarh(aes(xmin = ex3_ci_l, xmax = ex3_ci_u), height = 0,size = .5)+
  stat_cor(method = "pearson", label.x = 0.01, label.y = 0.99, aes(x = ex3, y = ex4), inherit.aes = F, size = 3)+
  coord_fixed()+
  xlim(0,1)+ylim(0,1)+
  xlab("Data Experiment 3")+
  ylab("Data Experiment 4")+
  theme_few(base_size = 10)+ 
  scale_color_viridis_d()

ggarrange(c1, c2, c3, ncol = 3, nrow = 1, legend = "right", common.legend = T , labels = c("A","B","C"), vjust = 1)
 
```

```{r ex4_results}
repl <- cor.test(
  pred_ex3%>%filter(Model == "Noisy RSA Model")%>%pull(Data),
  pred_ex4%>%filter(Model == "Noisy RSA Model", Prior == "Strong")%>%pull(Data)
  )

repl_r <- tidy(repl) %>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         estimate = round(estimate,2))


ex4_comp <- cor.test(
  pred_ex4%>%filter(Model == "Noisy RSA Model")%>%pull(mean),
  pred_ex4%>%filter(Model == "Noisy RSA Model")%>%pull(Data)
  )

ex4_comp_r <- tidy(ex4_comp) %>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         estimate = round(estimate,2))
  
```

As noted above, the strong prior condition was a direct replication of Experiment 3. Results from the two rounds of data collection were highly correlated (r = `r repl_r$estimate`, *p* `r repl_r$p.value`, see Fig. \ref{fig:plotmodelcomp}C). Across levels of prior manipulation, the data from Experiment 4 were highly correlated to the corresponding model predictions (r = `r ex4_comp_r$estimate`, *p* `r ex4_comp_r$p.value`, see Fig. \ref{fig:plotmodelcomp}B). Again, the pragmatics model provided a much better fit compared to the flat prior (BF = `r format(bf_ex4%>%filter(Comparison == "Pragmatic vs. No prior") %>%pull(BayesFactor),digits = 2)`) or prior only model (BF = `r format(bf_ex4%>%filter(Comparison == "Pragmatic vs. Prior only") %>%pull(BayesFactor), digits = 2)`). The inferred noise level in the pragmatics model was 0.28 (95% HDI: 0.24 - 0.32].

# Discussion

Language use and learning requires balancing different types of expectations about one’s interlocutor - both expectations about how speakers behave in general and specific expectations about how a particular speaker might behave in a particular context. Here we used a Bayesian pragmatics model to predict this integration process. Experiment 1 and 2 replicated previous studies showing that adult listeners expect speakers to produce utterances informatively and also with respect to common ground. We then combined the procedures from the first two experiments to study how listeners would integrate expectations. We used the results from Experiment 1 and 2 to specify model parameters that represented the two types of expectations, generating predictions about new behavior. Experiments 3 and 4 showed that both types of expectations influenced listeners inferences. Overall, listener behavior was accurately described by our model, suggesting that listeners trade-off flexibly between speaker specific and general pragmatic expectations. 

Notably, Experiment 3 also included situations in which the two expectations were in conflict. For example, in some trials the speaker expressed preference for object A, which was also the more frequent (less informative) object. In these situations, a majority of participants chose the preferred object as the referent (see also Fig. \ref{fig:plotexp3} preference--same speaker--incongruent). A simple explanation for this pattern might be that common ground manipulations were simply “stronger”, corroborated by the fact they produced higher rates of expected choice than the informativeness expectation when the two were presented in isolation (see Fig. \ref{fig:plotexp12}). In Experiment 4, however, the medium manipulation for novelty yielded numerically weaker results compared to the informativeness expectation in Experiment 1, and yet participants still selected the novel object above chance when the expectations were in conflict. Why is this? Because common ground is represented in our model as the listener’s prior distribution, speakers can reason about it in choosing their utterance. That is, in the mind of the listener, the speaker computes the effect of each utterance on a naive listener with shared common ground. Therefore, when prior interactions implicate one object as the more likely referent, the speaker reasons that this object will be the inferred referent of any semantically plausible utterance, even when the same utterance would point to a different object in the absence of prior information. 

<!-- In some cases, this constraining effect of prior knowledge can lead to counterintuitive predictions from RSA models [see @degen2015wonky for discussion and model extension].  -->

In our model, we treated common ground expectations as equivalent to more basic manipulations of contextual salience [e.g. in @frank2012predicting] and did not explicitly model the social-cognitive processes that give rise to these expectations. The interaction around the object prior to the test event simply increased the probability that this particular speaker will refer to the object subsequently. The same change could be brought about if one of the objects would be made perceptually more salient, for example by making it flash. In future work, it would be interesting to explore ways to model common ground expectations explicitly as well as to contrast perceptual and interactional salience.

<!-- Pragmatic reasoning is thought to play an important role in how children learn new words. Thus, a logical next step will be to extend the work presented here to children.  An ontogenetic study would also show to what extent the modelling approach taken here is suitable to address questions about developmental change. -->

This work integrates different perspectives on the study of pragmatic inference. Previous work focused either on general or speaker specific expectations. The methodological approach taken here illustrates how computational and experimental approaches can be used in conjunction to explicate theories of language use and learning.

<!-- \vspace{1em} \fbox{\parbox[b][][c]{7.3cm}{\centering Corresponding data and code are available at [masked for peer review]}} \vspace{1em} -->

<!-- \vspace{1em} \fbox{\parbox[b][][c]{7.3cm}{\centering Corresponding data and code are available at\ \url{https://github.com/manuelbohn/mcc}}} \vspace{1em} -->

# Acknowledgements

Masked for peer review.

<!-- MB received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 749229. -->

# References

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
