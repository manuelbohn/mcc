---
title             : "Pragmatic cue integration in adults’ and children’s inferences about novel word meanings"
shorttitle        : "Pragmatic cue integration"

author: 
  - name          : "Manuel Bohn"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Leipzig Research Center for Early Child Development, Jahnallee 59, 04109 Leipzig, Germany"
    email         : "manuel.bohn@uni-leipzig.de"
  - name          : "Michael Henry Tessler"
    affiliation   : "3"
  - name          : "Megan Merrick"
    affiliation   : "1"
  - name          : "Michael C. Frank"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
  - id            : "2"
    institution   : "Leipzig Research Center for Early Child Development, Leipzig University"
  - id            : "3"
    institution   : "Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology"

abstract: |
  Language is learned in complex social settings where listeners must reconstruct speakers’ intentend meanings from context. To navigate this challenge, children can use contextual or pragmatic reasoning to learn the meaning of unfamiliar words. One important challenge for pragmatic reasoning is that it often requires integrating information about the current utterance with broader common ground built up over the course of an interaction. Here we study this integration process. We isolate these two sources of pragmatic information and formalize both how they should be combined and how they might develop using a probabilistic model of conversational reasoning. We present a series of seven experiments with three- to five-year-old children and adults that suggest that these two information sources flexibly trade off with one another. A cue combination model accurately predicted empirical results for children and adults and provided a better fit compared with alternative models in which only one information source was considered. This work integrates distinct sets of findings regarding early language and suggests that pragmatic reasoning models can provide a quantitative framework for understanding developmental changes in language learning. 
  
keywords          : "language acquisition, social cognition, pragmatics, Bayesian modeling"
wordcount         : "X"

bibliography      : ["library.bib"]

csl               : pnas.csl

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---

```{r load_packages, include = FALSE}
# load packages and functions

knitr::opts_chunk$set(echo = F, include = F, out.width = "\\textwidth", fig.pos = "!h")

library("papaja")
library(tidyverse)
library(knitr)
library(ggthemes)
library(langcog)
library(rwebppl)
library(matrixStats)
library(coda)
#library(ggpubr)
library(lme4)
library(broom)
library(readxl)

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}

```


```{r data}
# load data files

# adults exp 1 
adult_ex1_data <- read_csv(file="../../stats/data/adult_ex1.csv")
# adults exp 2 novelty
adult_ex2_novelty_data <- read_csv(file="../../stats/data/adult_ex2_novelty.csv")
# adults exp 2 preference
adult_ex2_preference_data <- read_csv(file="../../stats/data/adult_ex2_preference.csv")
# adults ex3
adult_ex3_data <-read_csv(file="../../stats/data/adult_ex3.csv")
# prior strength manipulation experiments in experiment 4
adult_ex4_prior_data <-read_csv(file="../../stats/data/adult_ex4_prior.csv") 
# adults ex4
adult_ex4_data <-read_csv(file="../../stats/data/adult_ex4.csv") 

# children ex1
child_ex1_data <-read_csv(file="../../stats/data/child_ex1.csv")
# children ex2 preference
child_ex2_data <-read_csv(file="../../stats/data/child_ex2.csv") 
# children ex3
child_ex3_data <-read_csv(file="../../stats/data/child_ex3.csv") 
```

# Significance statement

In order to learn the meaning of a new word, children need to integrate multiple sources of information. Some information is provided by the utterance itself, but other contextual information accumulates over the course of a conversation and is stored in common ground. To understand how these sources are integrated with one another and how they might develop, we formalize this process using computational models of pragmatic reasoning. These models make quantitative predictions for different hypotheses about how integration proceeds. Across seven experiments, we find that the way in which preschoolers and adults integrate information is best described by a model that flexibly trades off between different sources of information. Taken together, this work presents an explicit theory of socially grounded language learning.

# Introduction

What someone means by an utterance is oftentimes not reducible to the words they used. It takes  pragmatic inference – context-sensitive reasoning about the speaker’s intentions - to recover the intended meaning [@grice1991studies, @levinson2000presumptive, @sperber2001relevance]. Contextual information comes in many forms. On the one hand, there is information provided by the utterance\footnote{We use the terms utterance, utterance-level information or utterance-level cues to capture all cues that the speaker provides for their intended meaning. This includes direct referential information in the form of actions such as pointing or gazing, semantic information in the form of conventional word meanings as well as pragmatic inferences that are licenced by the particular choice of words or actions.} itself. Competent language users expect each other to communicate in a cooperative way such that speakers produce utterances that are relevant and informative. Thus, semantic ambiguity can be resolved by reasoning about why the speaker produced this particular utterance [@grice1991studies, @clark1996using, @sperber2001relevance, @tomasello2008origins]. On the other hand, there is information provided by common ground (the body of knowledge and beliefs shared between interlocutors; [@bohn2018common, @clark1996using, @clark2015common]. Because utterances are embedded in a broader (ongoing) social interaction, common ground information serves as a background against which new utterances are interpreted, making some meanings for ambiguous terms more likely than others.	

Children learning their first language make inferences about intended meanings based on utterance-level and common-ground information both for language understanding and language learning [@bohn2019pervasive, @clark2009first, @tomasello2008origins]. Starting very early, infants expect adults to produce utterances in a cooperative way [@behne2005one], and expect language to be carrying information [@vouloumanos2012twelve]. By age two, children are sensitive to the informativeness of communication [@o2001two]. By age three children can use this expectation to make pragmatic inferences [@stiller2015ad, @yoon2019role] and to infer novel word meanings [@frank2014inferring]. And although older children continue to struggle with some complex pragmatic inferences until age five and beyond [@noveck2001children], an emerging consensus identifies these difficulties as stemming from difficulties reasoning about linguistic alternatives rather than pragmatic deficits [@skordos2016children, @horowitz2018trouble, @barner2011accessing]. Thus, children’s ability to reason about utterance-level pragmatics is present at least by ages three to five, and possibly substantially younger.

Evidence for the use of common ground information by young children is even stronger: Common ground information guides how infants produce non-verbal gestures and interpret ambiguous utterances [@bohn2018social, @saylor2011s]. For slightly older children, common ground – in the form of knowledge about discourse novelty, preferences, and even discourse expectations – also facilitates word learning [@akhtar1996role, @diesendruck2004two, @saylor2009preschoolers, @sullivan2019discourse]. 

All of these examples, however, highlight children’s use of a single pragmatic information source or cue. Harnessing multiple – potentially competing – cues poses a separate challenge. One aspect of this integration problem is how to balance common ground information that is built up over the course of an interaction against information gleaned from the current utterance. Much less is known about whether and how children – or even adults – combine these types of information. While many theories of pragmatic reasoning presuppose that both information sources are integrated, the nature of their relationship has typically not been specified. We address this challenge here. 

Recent innovations in probabilistic models of pragmatic reasoning provide a quantitative method for addressing the problem of integrating multiple sources of contextual information. This class of computational models, which are referred to as Rational Speech Act (RSA) models [@frank2012predicting, @goodman2016pragmatic] formalize the problem of language understanding as a special case of Bayesian social reasoning. A listener interprets an utterance by assuming it was produced by a cooperative speaker who had the goal to be informative. Being informative is defined as providing a message that would increase the probability of the listener recovering the speaker’s intended meaning in context. This notion of contextual informativeness captures the Gricean idea of cooperation between speaker and listener, and provides a first approximation to what we have described above as utterance-level pragmatic information. 

Listeners and speakers also enter into a conversation with assumptions about what is likely to be talked about, a reflection of the common ground shared between them. RSA models capture common ground information as a shared prior distribution over possible intended meanings. Thus, a natural locus for information integration within probabilistic models of pragmatic reasoning is the trade off between the prior probability of a meaning and the informativeness of the utterance. This trade off between contextual factors during word learning is a unique aspect of the word learning problem that is not addressed by other computational models of word learning, which have focused on learning from cross-situational, co-occurrence statistics [@fazly2010probabilistic, @frank2009using] or describing generalizations about word meaning [@xu2007word]. 

We make use of this framework to study pragmatic cue integration across development. To this end, we adapt a method used in perceptual cue integration studies [@ernst2002humans] predictions about conditions in which they either coincide or conflict. 

We start by replicating previous findings with adults showing that listeners make pragmatic inferences based on non-linguistic properties of utterances in isolation (Experiment 1). In separate experiments, we then show that adults make inferences based on common ground information (Experiment 2A and 2B). We use data from these experiments as parameters to generate a priori predictions from RSA models about how utterance and common ground information should be integrated. We consider three models that make different assumptions about the integration process: In the pragmatics model, the two information sources are integrated with one another; according to the flat prior model, participants focus only on the utterance information and in the prior only model, only common ground information is considered. We compare predictions from these models to new empirical data from experiments in which utterance and common ground information are manipulated simultaneously (Experiment 3 and 4). 

After successfully validating this approach with adults, we apply the same model-driven experimental procedure to children: We first show that they make pragmatic inferences based on utterance and common ground information separately (Experiment 5 and 6). Then we generate a priori model predictions and compare them to data from an experiment – parallel to Experiment 3 – in which both information sources have to be integrated (Experiment 7). To our knowledge, the strategy of independently estimating model parameters with separate groups and combining them via the model to make preregistered quantitative predictions about a new population has not been used before with children.

Taken together, this work makes two primary contributions: first, it shows that both adults and children integrate utterance-level (Gricean) and common-ground information flexibly. Second, it uses Bayesian data analysis within the RSA framework to provide a model for understanding the multiple loci for developmental change in complex behaviors like contextual communication.


```{r fig1, include = T, fig.align = "center", fig.cap = "Schematic experimental procedure with screenshots from the adult experiments. In all conditions, at test (bottom), the speaker ambiguously requested an object using a non-word (e.g. “dax”). Participants clicked on the object they thought the speaker referred to. Informativeness (Experiment 1, left) translated to making one object less frequent in context. Common ground (Experiment 2, middle) was manipulated by making one object preferred by or new to the speaker. Green plus signs represent utterances that expressed preference and red minus signs represent utterances that expressed dispreference (see main text for details). Experiment 3 (right) combined manipulations. One condition of Experiment 3 is shown here: preference - same speaker - incongruent.", out.width="450px"}
knitr::include_graphics("./figures/fig1.png")
```

# How do adults integrate contextual sources of information? 

## Inferences based on utterance and common ground information  (Experiments 1 and 2)

```{r ex1 results}
ex1t <- adult_ex1_data %>%
  group_by(condition, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(condition) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         p_value = t.test(unlist(correct), mu = 0.5)$p.value) %>%
  select(condition,df,t_value,p_value)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2))


# model
lm_ex1 <- glmer(correct ~ condition +
              (1 |id), 
              data = adult_ex1_data, 
              family = binomial,control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

#summary(lm_ex1)

ex1_r <- tidy(lm_ex1, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))
```

In Experiment 1, participants could learn which object a novel word referred to by assuming that the speaker communicated in an informative way [@frank2014inferring]. The speaker was located between two tables, one with two novel objects, A and B, and the other with only object A (Fig. 1A). When the speaker turned and pointed to the table with the two objects (A and B) and used a novel word to request one of them, participants could infer that the word referred to object B. This follows from the counter-factual inferences that, if the (informative) speaker had wanted to refer to object A, they would have pointed to the table with the single object (this being the least ambiguous way to refer to that object). In the control condition, both tables contained both objects and no inference could be made based on the speaker’s behavior. Participants selected object B above chance in the test condition (t(`r ex1t%>%filter(condition == "test")%>%pull(df)`) = `r ex1t%>%filter(condition == "test")%>%pull(t_value)`, *p* `r ex1t%>%filter(condition == "test")%>%pull(p_value)`) and more often compared to the control condition (*$\beta$* = `r ex1_r%>%filter(term == "conditiontest")%>%pull(estimate)`, se = `r ex1_r%>%filter(term == "conditiontest")%>%pull(std.error)`, *p* `r ex1_r%>%filter(term == "conditiontest")%>%pull(p.value)`). 
```{r ex2-results}
ex2tpref <- adult_ex2_preference_data %>%
  group_by(condition, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(condition) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         p_value = t.test(unlist(correct), mu = 0.5)$p.value) %>%
  select(condition,df,t_value,p_value)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2))


# model (maximally converging)
lm_ex2pref <- glmer(correct ~ condition +
              (1|id) + (1|agent), 
              data = adult_ex2_preference_data, 
              family = binomial,control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

#summary(lm_ex2pref)

ex2pref_r <- tidy(lm_ex2pref, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))

ex2tnov <- adult_ex2_novelty_data %>%
  group_by(condition, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(condition) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         p_value = t.test(unlist(correct), mu = 0.5)$p.value) %>%
  select(condition,df,t_value,p_value)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2))

# model
lm_ex2nov <- glmer(correct ~ condition +
              (condition|id), 
              data = adult_ex2_novelty_data, 
              family = binomial,control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

#summary(lm_ex2nov)

ex2nov_r <- tidy(lm_ex2nov, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))
```

In Experiments 2A and 2B, we tested if participants use common ground information that is specific to a speaker to identify the referent of a novel word [@akhtar1996role, @saylor2009preschoolers]. In Experiment 2A, the speaker expressed a preference for one of two objects (Fig. 1B, left). Later, the speaker used a novel word to request an object. Adults selected the preferred object above chance (t(`r ex2tpref%>%filter(condition == "same_speaker")%>%pull(df)`) = `r ex2tpref%>%filter(condition == "same_speaker")%>%pull(t_value)`, *p* `r ex2tpref%>%filter(condition == "same_speaker")%>%pull(p_value)`) and more so than in a control condition, where a different speaker, whose preferences were unknown, made the request ($\beta$ = `r ex2pref_r%>%filter(term == "conditionsame_speaker")%>%pull(estimate)`, se = `r ex2pref_r%>%filter(term == "conditionsame_speaker")%>%pull(std.error)`, *p* `r ex2pref_r%>%filter(term == "conditionsame_speaker")%>%pull(p.value)`). In Experiment 2B, common ground information came in the form of novelty (Fig 1B, right). First, the speaker encountered one object on one of the tables. Later, a second object appeared. When the same speaker then used a novel word to request an object, participants selected the new object above chance (t(`r ex2tnov%>%filter(condition == "same_speaker")%>%pull(df)`) = `r ex2tnov%>%filter(condition == "same_speaker")%>%pull(t_value)`, *p* `r ex2tnov%>%filter(condition == "same_speaker")%>%pull(p_value)`), and more often compared to when a different speaker (to whom both objects were equally new) made the request ($\beta$ = `r ex2nov_r%>%filter(term == "conditionsame_speaker")%>%pull(estimate)`, se = `r ex2nov_r%>%filter(term == "conditionsame_speaker")%>%pull(std.error)`, *p* `r ex2nov_r%>%filter(term == "conditionsame_speaker")%>%pull(p.value)`). Taken together, Experiments 1 and 2 confirmed that adults make pragmatic inferences based on information provided by the utterance as well as by common ground and provided quantitative estimates of the strength of these inferences for use in our model. 

## Model predictions for information integration evaluated against new data (Experiment 3).

We modeled the integration of utterance informativity and common ground as a process of socially-guided probabilistic inference, using the results of Experiments 1 and 2 to inform key parameters of a computational model. The Rational Speech Act (RSA) model architecture introduced by [@frank2012predicting] encodes conversational reasoning through the perspective of a listener (“he” pronoun) who is trying to decide on the intended meaning of the utterance he heard from the speaker (“she” pronoun). The basic idea is that the listener combines his uncertainty about the speaker’s intended meaning - a prior distribution over referents P(r) - with his generative model of how the utterance was produced: a speaker trying to convey information to him. To adapt this model to the word learning context, we enrich this basic architecture with a mechanism for expressing uncertainty about the meanings of words (lexical uncertainty) - a prior distribution over lexica P(L) [@bergen2016pragmatic]. 

$$P_{L}(r, \mathcal{L}|u)\propto P_{S}(u|r, \mathcal{L}) \cdot P( \mathcal{L}) \cdot P(r)$$

In the above equation, the listener is trying to jointly resolve the speaker’s intended referent r and the meaning of words (thus learning the lexicon $\mathcal{L}$). He does this by imagining what a rational speaker would say, given the referent they are trying to communicate and a lexicon. The speaker is an approximately rational Bayesian actor (with degree of rationality alpha), who produces utterances as a function of their informativity. The space of utterances the speaker could produce depends upon the lexicon $P(u|\mathcal{L})$; simply put, the speaker labels objects with the true labels under a given lexicon L (see SI Appendix for details):

$$P_{S}(u|r,\mathcal{L})\propto Informativity(u;r)^\alpha \cdot P(u|\mathcal{L})$$

The informativity of an utterance for a referent is taken to be the probability with which a naive listener, who only interprets utterances according to their literal semantics, would select a particular referent given an utterance.

$$Informativity(u; r) = P(r|u) \propto P(r) \cdot \mathcal{L}_{point}$$

The speaker’s possible utterances are pairs of linguistic and non-linguistic signals, namely labels and points. Because the listener does not know the lexicon, the informativity of an utterance comes from the speaker’s point, the meaning of which is encoded in $\mathcal{L}_{point}$ and is simply a truth-function checking whether or not the referent is at the location picked out by the speaker’s point. Though the speaker makes their communicative decision assuming the listener does not know the meaning of the labels, we assume that in addition to a point, the speaker produces a label consistent with their own lexicon $\mathcal{L}$, described by $P(u|\mathcal{L})$ (see SI Appendix for modeling details).

This computational model provides a natural avenue to formalize quantitatively how informativeness and common ground trade-off during word learning. As mentioned above, the common ground shared between speaker and listener plays the role of the listener’s prior distribution over meanings, or types of referents, that the speaker might be referring to and which we posit depends on prior interactions around the referents in the present context (e.g., preference or novelty; Experiment 2A and B). We use the results from Experiment 2 to specify this distribution. The in-the-moment, contextual informativeness of the utterance is captured in the likelihood term, whose value depends on the rationality parameter $\alpha$. Assumptions about rationality may change depending on context and we therefore used the data from Experiment 1 to specify $\alpha$ (see SI Appendix for details about these parameters). 	

The model generates predictions for situations in which utterance and common ground expectations are jointly manipulated (Fig. 1C - see SI Appendix for additional details and a worked example of how predictions were generated). In addition to the parameters fit to the data from previous experiments, we include an additional noise parameter to account for responses better explained by a process of random guessing than by pragmatics; we estimate this parameter from the observed data (Experiment 3). Including the noise parameter greatly improved the model fit to the data (see SI Appendix for details). We did not pre-register the inclusion of a noise parameter for Experiment 3 but did so for all subsequent experiments.
```{r ex3 GLMM, cache=F}
# model 
lm_ex3 <- glmer(correct_inf ~ common_ground_manipulation*speaker*alignment + (alignment|id), 
              data = adult_ex3_data, family = binomial, 
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

#summary(lm_ex3)

lm_ex3_r <- tidy(lm_ex3, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))

```

```{r model comparison ex3}
ex3_bf <- bind_rows(
  readRDS("../../stats/saves/ex3_prior_only_model_noise_loglike.rds") %>% mutate(model = "prior_only_noise"),
  readRDS("../../stats/saves/ex3_pragm_model_noise_loglike.rds") %>% mutate(model = "pragmatic_noise"),
  readRDS("../../stats/saves/ex3_flat_prior_model_noise_loglike.rds") %>% mutate(model = "flat_prior_noise")
)%>%
  group_by(model)%>%
  summarize(logP = logSumExp(value))%>%
  bind_rows(readRDS("../../stats/saves/ex3_pragm_model_loglike.rds") %>% mutate(model = "pragmatic_parameter_free"))%>%
  spread(model, logP) %>%
  mutate("pragmatic_noise_pragmatic_parameter_free" = exp(pragmatic_noise - pragmatic_parameter_free),
         "pragmatic_noise_prior_only_noise" = exp(pragmatic_noise - prior_only_noise),
         "pragmatic_noise_flat_prior_noise" = exp(pragmatic_noise - flat_prior_noise),
         "prior_only_noise_flat_prior_noise" = exp(prior_only_noise - flat_prior_noise)) %>%
  select(-pragmatic_parameter_free,-pragmatic_noise, -flat_prior_noise, -prior_only_noise)


ex3_pragm_model_noise_param <- readRDS("../../stats/saves/ex3_pragm_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))%>%
  round(digits = 2)
```

In Experiment 3, we combined the procedures of Experiment 1 and 2A or 2B. The test setup was identical to Experiment 1, however, before making a request, the speaker interacted with the objects so that some of them were preferred by or new to them (Fig. 1C). We discuss and visualize the results as the proportion with which participants chose the more informative object (i.e., the object that would be the more informative referent when only utterance information is considered). Participants distinguished between congruent and incongruent trials when the speaker remained the same, as evidenced by the fit of a generalized linear mixed effects model (model term: `alignment x speaker`; $\beta$ = `r lm_ex3_r%>%filter(term == "speakersame_speaker:alignmentincongruent")%>%pull(estimate)`, se = `r lm_ex3_r%>%filter(term == "speakersame_speaker:alignmentincongruent")%>%pull(std.error)`, *p* `r lm_ex3_r%>%filter(term == "speakersame_speaker:alignmentincongruent")%>%pull(p.value)`).

Participants’ average responses were highly correlated with the model’s predictions in each condition (Fig. 2A). To test whether participants in fact balanced both information sources, we compared the pragmatics model to two alternative models: the *flat prior model*, which ignores common ground information and the *prior only model*, which ignores utterance information. Model fit was considerably better for the pragmatics model compared to the flat prior model (Bayes Factor (BF) = `r format(ex3_bf%>%pull(pragmatic_noise_flat_prior_noise), digits = 2)`) or the prior only model (BF = `r format(ex3_bf%>%pull(pragmatic_noise_prior_only_noise), digits = 2)`), suggesting that participants considered and integrated both sources of information. The estimated proportion of random responses according to the pragmatics model was `r ex3_pragm_model_noise_param%>%pull(mean)` (95% Highest Density Interval (HDI): `r ex3_pragm_model_noise_param%>%pull(ci_lower)` - `r ex3_pragm_model_noise_param%>%pull(ci_upper)`). This value was substantially lower for the pragmatics model compared to the alternative models (see SI Appendix), lending additional support to the conclusion that the pragmatics model better captured the behavioral data. Rather than explaining systematic structure in the data, the alternative models achieved their best fit only by assuming a very high level of noise.

```{r fig2, include = T, fig.align = "center", fig.cap = "Correlation between model predictions and data. (A) Experiment 3, (B) Experiment 4, (C) 3-year-olds in Experiment 7 and (D) 4-year-olds in Experiment 7. Coefficients and p-values are based on Pearson correlation statistics. Error bars represent  95\\% HDIs.", out.width="450px"}
knitr::include_graphics("./figures/fig2.png")
```

## Replication and extension to different levels of common ground information (Experiment 4).

```{r model comparison ex4}
ex4_bf <- bind_rows(
  readRDS("../../stats/saves/ex4_prior_only_model_noise_loglike.rds") %>% mutate(model = "prior_only_noise"),
  readRDS("../../stats/saves/ex4_pragm_model_noise_loglike.rds") %>% mutate(model = "pragmatic_noise"),
  readRDS("../../stats/saves/ex4_flat_prior_model_noise_loglike.rds") %>% mutate(model = "flat_prior_noise")
)%>%
  group_by(model)%>%
  summarize(logP = logSumExp(value))%>%
  spread(model, logP) %>%
  mutate("pragmatic_noise_prior_only_noise" = exp(pragmatic_noise - prior_only_noise),
         "pragmatic_noise_flat_prior_noise" = exp(pragmatic_noise - flat_prior_noise),
         "prior_only_noise_flat_prior_noise" = exp(prior_only_noise - flat_prior_noise)) %>%
  select(-pragmatic_noise, -flat_prior_noise, -prior_only_noise)
ex4_pragm_model_noise_param <- readRDS("../../stats/saves/ex4_pragm_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))%>%
  round(digits = 2)
```

To test if our model makes accurate predictions for different combinations, we first replicated and then extended the results of Experiment 3 to a broader range of experimental conditions. Specifically, we manipulated the strength of the common ground information (strong, medium and weak manipulation) by changing  the way the speaker interacted with the objects prior to the request We ran a total of 20 conditions, including a direct replication of Experiment 3 (see SI Appendix for details).

Model predictions from the pragmatics model were again highly correlated with the average response in each condition (Fig. 2B). We evaluated model fit for the same models as in Experiment 3 and found again that the pragmatics model fit the data much better compared to the flat prior (BF = `r format(ex4_bf%>%pull(pragmatic_noise_flat_prior_noise), digits = 2)`) or the prior only model (BF = `r format(ex4_bf%>%pull(pragmatic_noise_prior_only_noise), digits = 2)`). The inferred level of noise based on the data for the pragmatics model was `r ex4_pragm_model_noise_param%>%pull(mean)` (95% HDI: `r ex4_pragm_model_noise_param%>%pull(ci_lower)` - `r ex4_pragm_model_noise_param%>%pull(ci_upper)`), which was similar to Experiment 3 and again lower compared to the alternative models (see SI Appendix).

# Do children integrate contextual information?

The previous section showed that competent language users flexibly integrate information during pragmatic word learning. Do children make use of multiple information sources during word learning as well? When does this integration emerge developmentally? While many verbal theories of language learning imply that this integration takes place, the actual process has neither been described in detail nor tested. Here we provide an explanation in the form of our pragmatics model and test if it is able to capture children’s word learning. Embedded in the assumptions of the model is the idea that developmental change is change in the strength of the individual inferences, leading to a change in the strength of the integrated inference. As a starting point, our model assumes developmental continuity in the integration process itself, though this assumption could be called into question by a poor model fit.

## Inferences based on utterance and common ground information (Experiment 5 and 6)

The study for children followed the same general pattern as the one for adults. We generated model predictions for how information should be integrated by first measuring children’s ability to use utterance (informativeness) and common ground (preference) information in isolation when making pragmatic inferences. We then adapted our model to study developmental change: We sampled children continuously between 3.0 and 5.0 years of age – a time in which children have been found to make the kind of pragmatic inferences we studied here [@bohn2019pervasive, frank2014inferring] - and generated model predictions for the average developmental trajectory in each condition\footnote{For Experiment 5 and 6, we also tested two-year-olds but did not find sufficient evidence that they use utterance and/or common ground information in the tasks we used to justify investigating their ability to integrate the two.}.

```{r child ex1 and 2 results}
#experiment 1
child_ex1t <- child_ex1_data %>%
  group_by(age_bin, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(age_bin) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         p_value = t.test(unlist(correct), mu = 0.5)$p.value) %>%
  select(age_bin,df,t_value,p_value)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2))


child_ex1_lm_data <- child_ex1_data%>%
  mutate(age_num = scale(age_num, center = TRUE, scale = TRUE))

# GLMM
child_ex1_lm <- glmer(correct ~ age_num + (1 | id), 
              data = child_ex1_lm_data, family = binomial, 
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

#summary(child_ex1_lm)

child_lm_ex1_r <- tidy(child_ex1_lm, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))

# experiment 2
child_ex2t <- child_ex2_data %>%
  group_by(condition,age_bin, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(condition,age_bin) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         p_value = t.test(unlist(correct), mu = 0.5)$p.value) %>%
  select(age_bin,df,t_value,p_value)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2))

# GLMM

child_ex2_lm_data <- child_ex2_data%>%
  mutate(age_num = scale(age_num, center = TRUE, scale = TRUE))

child_ex2_lm <- glmer(correct ~ age_num*condition + (condition | id) + (1 | agent), 
              data = child_ex2_lm_data, family = binomial, 
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

summary(child_ex2_lm)

child_lm_ex2_r <- tidy(child_ex2_lm, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))

```

Experiment 5 was analogous to Experiment 1 for adults. To compare children’s performance to chance level, we binned age by year. Four-year-olds selected the more informative object (i.e. the object that was unique to the location the speaker turned to) above chance (t(`r child_ex1t%>%filter(age_bin =="4")%>%pull(df)`) = `r child_ex1t%>%filter(age_bin =="4")%>%pull(t_value)`, *p* `r child_ex1t%>%filter(age_bin =="4")%>%pull(p_value)`). Three-year-olds, on the other hand, did not (t(`r child_ex1t%>%filter(age_bin =="3")%>%pull(df)`) = `r child_ex1t%>%filter(age_bin =="3")%>%pull(t_value)`, *p* `r child_ex1t%>%filter(age_bin =="3")%>%pull(p_value)`). Consequently, when we fit a GLMM to the data with age as a continuous predictor, performance increased with age ($\beta$ = `r child_lm_ex1_r%>%filter(term == "age_num")%>%pull(estimate)`, se = `r child_lm_ex1_r%>%filter(term == "age_num")%>%pull(std.error)`, *p* `r child_lm_ex1_r%>%filter(term == "age_num")%>%pull(p.value)`). Thus, children’s ability to use utterance information in a word learning context increased with age.

In Experiment 6, we assessed whether children use common ground information to identify the referent of a novel word. We tested children with the novelty as well as the preference manipulation but found little evidence that children distinguished between requests made by the same speaker or a different speaker in the case of novelty. Since our focus was on how children selectively integrate the two sources of information, we therefore dropped this manipulation and focused on preference for the remainder of the study.

For preference, four-year-olds selected the preferred object above chance when the same speaker made the request (t(`r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="4")%>%pull(df)`) = `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="4")%>%pull(t_value)`, *p* `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="4")%>%pull(p_value)`), whereas three-year-olds did not (t(`r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="3")%>%pull(df)`) = `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="3")%>%pull(t_value)`, *p* `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="3")%>%pull(p_value)`). However, when we fit a GLMM to the data with age as a continuous predictor, we found an effect of speaker identity ($\beta$ = `r child_lm_ex2_r%>%filter(term == "conditionsame_speaker")%>%pull(estimate)`, se = `r child_lm_ex2_r%>%filter(term == "conditionsame_speaker")%>%pull(std.error)`, *p* `r child_lm_ex2_r%>%filter(term == "conditionsame_speaker")%>%pull(p.value)`) but no effect of age ($\beta$ = `r child_lm_ex2_r%>%filter(term == "age_num")%>%pull(estimate)`, se = `r child_lm_ex2_r%>%filter(term == "age_num")%>%pull(std.error)`, *p* `r child_lm_ex2_r%>%filter(term == "age_num")%>%pull(p.value)`) or interaction between speaker identity and age ($\beta$ = `r child_lm_ex2_r%>%filter(term == "age_num:conditionsame_speaker")%>%pull(estimate)`, se = `r child_lm_ex2_r%>%filter(term == "age_num:conditionsame_speaker")%>%pull(std.error)`, *p* `r child_lm_ex2_r%>%filter(term == "age_num:conditionsame_speaker")%>%pull(p.value)`4). Thus, children across the age range used common ground information to infer the referent of a novel word.

## Developmental model predictions evaluated against new data (Experiment 7)

We used the measurements from Experiment 5 and 6 to specify the strength of informativity, $\alpha$, and common ground in the pragmatics model. Instead of inferring a single value we inferred the intercept and slope for each parameter that best described the developmental trajectory in the data of Experiment 5 and 6. These parameter settings were then used to generate age sensitive model predictions in 2 (same or different speaker) x 2 (congruent or incongruent) = 4 conditions. As for adults, all models included a noise parameter, which was estimated based on the data.

```{r child ex3 GLMM, cache = F}
child_ex3_lm_data <- child_ex3_data%>%
  mutate(age_num = scale(age_num, center = TRUE, scale = TRUE))

child_ex3_lm <- glmer(correct_inf ~ age_num*speaker*alignment 
      + (speaker+alignment | id), 
      family = "binomial",
      data = child_ex3_lm_data,
      control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

summary(child_ex3_lm)

child_lm_ex3_r <- tidy(child_ex3_lm, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))
```

In Experiment 7, we combined the procedures of Experiment 5 and 6 and collected new data from children between 3.0 and 5.0 years of age in each of the four conditions (Fig. 1C). Children’s propensity to differentiate between congruent and incongruent trials for the same or a different speaker increased with age (model term: `age x alignment x speaker`; $\beta$ = `r child_lm_ex3_r%>%filter(term == "age_num:speakersame_speaker:alignmentincongruent")%>%pull(estimate)`, se = `r child_lm_ex3_r%>%filter(term == "age_num:speakersame_speaker:alignmentincongruent")%>%pull(std.error)`, *p* `r child_lm_ex3_r%>%filter(term == "age_num:speakersame_speaker:alignmentincongruent")%>%pull(p.value)`).

```{r child model comparison}

child_ex3_bf <- readRDS("../../stats/saves/child_ex3_model_comparison.rds") %>%
  group_by(parameter)%>%
  spread(model, logP) %>%
  mutate("pragmatic_flat_prior" = exp(pragmatic - flat_prior),
         "pragmatic_prior_only" = exp(pragmatic - prior_only),
         "flat_prior_prior_only" = exp(flat_prior - prior_only)) %>%
  select(-pragmatic,-flat_prior, -prior_only)


child_ex3_pragm_model_noise_param <- readRDS("../../stats/saves/child_ex3_pragm_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))
```

Our modeling results suggest that children flexibly integrate both common ground and informativity information, and that this integration process is accurately captured by the pragmatics model at least for four-year-olds. For the correlational analysis, we binned model predictions and data by year. There was a substantial correlation between the predicted and measured average response for four-year-olds, but less so for three-year-olds (Fig. 2C and D). One of the reasons for the latter was the low variation between conditions. For the model comparison, we treated age continuously. As with adults, we found a much better model fit for the pragmatics model compared to the flat prior (BF = `r format(child_ex3_bf%>%filter (parameter=="noise")%>%pull(pragmatic_flat_prior), digits = 2)`) or the prior only model (BF = `r format(child_ex3_bf%>%filter (parameter=="noise")%>%pull(pragmatic_prior_only), digits = 2)`). The inferred level of noise based on the data for the pragmatics model was `r child_ex3_pragm_model_noise_param%>%pull(mean)` (95% HDI: `r child_ex3_pragm_model_noise_param%>%pull(ci_lower)` - `r child_ex3_pragm_model_noise_param%>%pull(ci_upper)`), which was lower compared to the alternative models considered but numerically higher than that of adults (see SI Appendix). 

The high level of inferred noise moved the model predictions for children in all conditions close to chance level. We therefore compared two additional sets of models with different parameterizations that emphasized differences between conditions in the model predictions more (see SI Appendix). This analysis was not pre-registered. Parameter free models did not include a noise parameter and developmental noise models allowed the noise parameter to change with age. In each case, the pragmatics model provided a better fit compared to the alternative models (flat prior: parameter free BF = `r format(child_ex3_bf%>%filter (parameter=="parameter free")%>%pull(pragmatic_flat_prior), digits = 2)`, developmental noise BF = `r format(child_ex3_bf%>%filter (parameter=="developmental noise")%>%pull(pragmatic_flat_prior), digits = 2)`; prior only: parameter free BF = `r format(child_ex3_bf%>%filter (parameter=="parameter free")%>%pull(pragmatic_prior_only), digits = 2)`, developmental noise BF = `r format(child_ex3_bf%>%filter (parameter=="developmental noise")%>%pull(pragmatic_prior_only), digits = 2)`). 

# Discussion

Integrating multiple sources of information is an integral part of human communication [@tanenhaus1995integration]. To infer the intended meaning of an utterance, listeners must combine their knowledge of communicative conventions (semantics and syntax) with social expectations about their interlocutor. This integration is especially vital in early language learning, and the different varieties of pragmatic information are among the most important sources [@bohn2019pervasive]. But how are different cues integrated during word learning? Here we used a Bayesian pragmatics model to formalize this integration process. We studied how utterance-level (Gricean expectations) about informative communication are integrated with common ground information that follows from prior interactions with the speaker. Adults’ and children’s learning was best predicted by a model in which both sources of information traded-off flexibly. Alternative models that considered only one source of information made substantially worse predictions.

All of the models we compared here integrated some explicit structure, rather than (for example) simply weighing expectations by some ratio. We made this decision because we wanted to make predictions within a framework in which the models were models of the task, rather than simply models of the data. That is, inferences are not computed separately by the modeler and specified as inputs to a regression model, but instead are the results of an integrated process that operates over a (schematic) representation of the experimental stimuli. Further, our models are variants derived from the broader RSA framework, which has been integrated into larger systems for language learning in context [@wang2016learning]. 

We conceptualized developmental change as age related changes in the propensity to make the individual inferences. That is, while the degree to which listeners expect speakers to be informative or follow common ground changes with age, the process by which expectations are integrated remains the same. However, other developmental models are also worth exploring in future work; one possible candidate would be a model in which the integration process itself changes with age. Our model did not successfully describe three-year-olds’ inferences; thus, it is possible that they were not able to integrate information sources. But our findings are also consistent with a simpler explanation, namely that the overall weaker responses we observed in the independent measurement experiments (Experiments 5 and 6), combined with some noise in responding, led the younger children to appear relatively random in their responses. 

Studying how multiple types of pragmatic cues are balanced contributes to a more comprehensive understanding of word learning. In the current study, participants inferred the referent by integrating non-linguistic cues (speakers pointing to a table) with assumptions about speaker informativeness and common ground information, going beyond previous experimental work in measuring how these information sources were combined. The real learning environment is far richer than what we captured in our experimental design, however. For example, in addition to multiple layers of social information, children can rely on semantic and syntactic features of the utterances as cues to meaning [@clark1973s; @abend2017bootstrapping; @gleitman1990structural]. Across development, children learn to recruit these different sources of information and integrate them. RSA models allow for the inclusion of semantic information as part of the utterance [@bergen2016pragmatic] and it will be a fruitful avenue for future research to model the integration of linguistic and pragmatic information across development. 

More broadly, our work here shows how computational models of language comprehension can be used as powerful tools to explicate and test hypotheses about information integration. Furthermore, we took a first step towards integrating developmental change into this theoretical framework. 

# Methods

All experimental procedures, sample sizes and statistical analysis were pre-registered (https://osf.io/u7kxe/). Experimental stimuli, data files and analysis scripts are freely available in an online repository (https://github.com/manuelbohn/mcc). 

## Participants

Adult participants were recruited via Amazon Mechanical Turk (MTurk) and received payment equivalent to an hourly wage of ~ \$9. Experiment 1 and each manipulation of Experiment 2 had N = 40 participants. Sample size in Experiment 3 was *N* = `r length(unique(adult_ex3_data$id))`. *N* = `r length(unique(adult_ex4_prior_data$id))` participated in the experiments to measure the strong, medium and weak preference and novelty manipulations. Finally, experiment 4 had *N* = `r length(unique(adult_ex4_data$id))` participants. 

Children were recruited from the floor of the Children’s Discovery Museum in San Jose, California, USA. Parents gave informed consent and provided demographic information. We collected data from a total of 243 children between 3.0 and 5.0 years of age. We excluded 15 children due to less than 75% of reported exposure to English, five because they responded incorrectly on 2/2 training trials, three because of equipment malfunction, and two because they quit before half of the test trials were completed. The final sample size in each experiment was as follows: *N* = `r length(unique(child_ex1_data$id))` (41 girls, mean age = `r round(mean(child_ex1_data$age_num),2)`) in Experiment 5,  *N* = `r length(unique(child_ex2_data$id))` (28 girls, mean age = `r round(mean(child_ex2_data$age_num),2)`) in Experiment 6 and *N* = `r length(unique(child_ex3_data$id))` (54 girls, mean age = `r round(mean(child_ex3_data$age_num),2)`) in Experiment 7.

## Materials

All experiments were framed as games in which participants would learn words from animals. They were implemented in HTML/JavaScript as a website. Adults were directed to the website via MTurk and responded by clicking objects. Children were guided through the game by an experimenter and responded by touching objects on the screen of an iPad tablet [@frank2016using]. For each animal character, we recorded a set of utterances (one native English speaker per animal) that were used to provide information and make requests. All experiments started with an introduction to the animals and two training trials. Subsequent test trials in each condition were presented in a random order. Detailed experimental procedures for each experiment can be found in the SI Appendix.


## Analysis

All analyses were run in R [@R-base]. GLMMs were fit via the function `glmer` from the package `lme4` [@R-lme4] and had a maximal random effect structure conditional on model convergence. Probabilistic models and model comparisons were implemented in WebPPL [@dippl] using the r package `rwebppl` [@R-rwebppl]. Bayes Factors for model comparisons were based on marginal likelihoods of each model given the data. Details on models can be found in the supplementary information.

# Acknowledgements

MB received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement no. 749229. MCF was supported by a Jacobs Foundation Advanced Research Fellowship and the Zhou Fund for Language and Cognition. We thank Jacqueline Quirke and Sabina Zacco for help with the data collection and Bria Long and Gregor Kachel for comments on an earlier version of the paper.


\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup






