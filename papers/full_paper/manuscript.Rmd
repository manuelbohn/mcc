---
title             : "Predicting pragmatic cue integration in adults’ and children’s inferences about novel word meanings"
shorttitle        : "Pragmatic cue integration"

author: 
  - name          : "Manuel Bohn"
    affiliation   : "1,2,3"
    corresponding : yes    # Define only one corresponding author
    address       : "Leipzig Research Center for Early Child Development, Jahnallee 59, 04109 Leipzig, Germany"
    email         : "manuel.bohn@uni-leipzig.de"
  - name          : "Michael Henry Tessler"
    affiliation   : "4"
  - name          : "Megan Merrick"
    affiliation   : "1"
  - name          : "Michael C. Frank"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
  - id            : "2"
    institution   : "Leipzig Research Center for Early Child Development, Leipzig University"
  - id            : "3"
    institution   : "Department of Comparative Cultural Psychology, Max Planck Institute for Evolutionary Anthropology"
  - id            : "4"
    institution   : "Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology"


abstract: |
  Language is learned in complex social settings where listeners must reconstruct speakers’ intentend meanings from context. To navigate this challenge, children can use pragmatic reasoning to learn the meaning of unfamiliar words. One important challenge for pragmatic reasoning is that it requires integrating multiple information sources. Here we study this integration process. We isolate two sources of pragmatic information and, using a probabilistic model of conversational reasoning, formalize both how they should be combined and how this process might develop. We use this model to generate quantitative predictions, which we test against new behavioral data from three- to five-year-old children (N = 243) and adults (N = 694). Results show close numerical alignment between model predictions and data. This work integrates distinct sets of findings regarding early language and suggests that pragmatic reasoning models can provide a quantitative framework for understanding developmental changes in language learning.  
  
keywords          : "language acquisition, social cognition, pragmatics, Bayesian modeling, common ground"

bibliography      : ["library.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---

```{r load_packages, include = FALSE}
# load packages and functions

knitr::opts_chunk$set(echo = F, include = F, out.width = "\\textwidth", fig.pos = "!h")

library("papaja")
library(tidyverse)
library(knitr)
library(ggthemes)
library(langcog)
library(rwebppl)
library(matrixStats)
library(coda)
#library(ggpubr)
library(lme4)
library(broom)
library(readxl)
library(lsr)

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}

```


```{r data}
# load data files

# adults exp 1 
adult_ex1_data <- read_csv(file="../../stats/data/adult_ex1.csv")
# adults exp 2 novelty
adult_ex2_novelty_data <- read_csv(file="../../stats/data/adult_ex2_novelty.csv")
# adults exp 2 preference
adult_ex2_preference_data <- read_csv(file="../../stats/data/adult_ex2_preference.csv")
# adults ex3
adult_ex3_data <-read_csv(file="../../stats/data/adult_ex3.csv")
# prior strength manipulation experiments in experiment 4
adult_ex4_prior_data <-read_csv(file="../../stats/data/adult_ex4_prior.csv") 
# adults ex4
adult_ex4_data <-read_csv(file="../../stats/data/adult_ex4.csv") 

# children ex1
child_ex1_data <-read_csv(file="../../stats/data/child_ex1.csv")
# children ex2 preference
child_ex2_data <-read_csv(file="../../stats/data/child_ex2.csv") 
# children ex3
child_ex3_data <-read_csv(file="../../stats/data/child_ex3.csv") 
```

# Introduction 

What someone means by an utterance is oftentimes not reducible to the words they used. It takes  pragmatic inference – context-sensitive reasoning about the speaker’s intentions - to recover the intended meaning [@grice1991studies; @levinson2000presumptive; @sperber2001relevance]. Contextual information comes in many forms. On the one hand, there is information provided by the utterance\footnote{We use the terms utterance, utterance-level information or utterance-level cues to capture all cues that the speaker provides for their intended meaning. This includes direct referential information in the form of pointing or gazing, semantic information in the form of conventional word meanings as well as pragmatic inferences that are licenced by the particular choice of words or actions.} itself. Competent language users expect each other to communicate in a cooperative way such that speakers produce utterances that are relevant and informative. Thus, semantic ambiguity can be resolved by reasoning about why the speaker produced this particular utterance [@grice1991studies; @clark1996using; @sperber2001relevance; @tomasello2008origins]. On the other hand, there is information provided by common ground (the body of mutually shared knowledge and beliefs between interlocutors)[@bohn2018common; @clark1996using; @clark2015common]. Because utterances are embedded in common ground, pragmatic reasoning in context always requires information integration. But how does integration proceed? And how does it develop? Verbal theories assume that information is integrated and that this process develops but do not specify how. We bridge this gap by formalizing information integration and development in a probabilistic model of pragmatic reasoning.

Children learning their first language make inferences about intended meanings based on utterance-level and common-ground information both for language understanding and language learning [@bohn2019pervasive; @clark2009first; @tomasello2008origins]. Starting very early, infants expect adults to produce utterances in a cooperative way [@behne2005one], and expect language to be carrying information [@vouloumanos2012twelve]. By age two, children are sensitive to the informativeness of communication [@o2001two]. By age three children can use this expectation to make pragmatic inferences [@stiller2015ad; @yoon2019role] and to infer novel word meanings [@frank2014inferring]. And although older children continue to struggle with some complex pragmatic inferences until age five and beyond [@noveck2001children], an emerging consensus identifies these difficulties as stemming from difficulties reasoning about linguistic alternatives rather than pragmatic deficits [@skordos2016children; @horowitz2018trouble; @barner2011accessing]. Thus, children’s ability to reason about utterance-level pragmatics is present at least by ages three to five, and possibly substantially younger.

Evidence for the use of common ground\footnote{Common ground has traditionally been defined in recursive terms: in order to be part of common ground, some piece of information has to be not just known to both interlocutors but also known to both to be shared between them [@clark1996using]. Numerous studies probed the role of sharedness of information and found that it plays a critical role in communicative interactions [@brown2009partner;@hanna2003effects; @mozuraitis2015privileged]. Based on this literature, one might argue that the term common ground should be restricted to describe situations in which the sharedness aspect is directly tested. However, most of this work is focused on online perspective taking. In this paper, we use the term common ground to refer to shared information that is built up over the course of an interaction - something that is supposedly easier for children [@matthews2006effect]. We assume that the consequence of a direct interaction (with matching perspectives) between the speaker and the listener is that information is not just known to both interlocutors but also assumed to be shared between them [see also @bohn2018common]} information by young children is even stronger: Common ground information guides how infants produce non-verbal gestures and interpret ambiguous utterances [@bohn2018social; @saylor2011s]. For slightly older children, common ground – in the form of knowledge about discourse novelty, preferences, and even discourse expectations – also facilitates word learning [@akhtar1996role; @bohn_le_peloquin_koymen_frank_2020; @saylor2009preschoolers; @sullivan2019discourse]. 

These examples, however, highlight children’s use of a single pragmatic information source or cue. Harnessing multiple – potentially competing – pragmatic cues poses a separate challenge. One aspect of this integration problem is how to balance common ground information that is built up over the course of an interaction against information gleaned from the current utterance. Much less is known about whether and how children combine these types of information. Developmental studies that look at the integration of multiple information sources more generally find that children are sensitive to multiple sources from early on [@graham2017words; @grosse201021; @khu2020preschoolers; @matthews2006effect; @nilsen2009preschoolers; @ganea2007infants]. For example, in a classic study, Nadig and Sedivy [-@nadig2002evidence] found that children rapidly integrate information provided in an utterance (a particular referring expression) with the speaker’s perspective (the objects the speaker can see). However, the information sources to be integrated in these studies are not all pragmatic in nature. Children’s ability to pick out a referent following a noun reflects their linguistic knowledge and not necessarily their ability to reason about the speaker’s intention in context. As a consequence, this work does not speak to the question of how and if listeners integrate different forms of *pragmatic* information. Thus, while many theories of pragmatic reasoning presuppose that pragmatic information sources are integrated, the nature of their relationship has typically not been specified. 

Recent innovations in probabilistic models of pragmatic reasoning provide a quantitative method for addressing the problem of integrating multiple sources of contextual information. This class of computational models, which are referred to as Rational Speech Act (RSA) models [@frank2012predicting; @goodman2016pragmatic] formalize the problem of language understanding as a special case of Bayesian social reasoning. A listener interprets an utterance by assuming it was produced by a cooperative speaker who had the goal to be informative. Being informative is defined as providing a message that would increase the probability of the listener recovering the speaker’s intended meaning in context. This notion of contextual informativeness captures the Gricean idea of cooperation between speaker and listener, and provides a first approximation to what we have described above as utterance-level pragmatic information. 

RSA models capture common ground information as a shared prior distribution over possible intended meanings. Thus, a natural locus for information integration within probabilistic models of pragmatic reasoning is the trade off between the prior probability of a meaning and the informativeness of the utterance. This trade off between contextual factors during word learning is a unique aspect that is not addressed by other computational models of word learning, which have focused on learning from cross-situational, co-occurrence statistics [@fazly2010probabilistic; @frank2009using] or describing generalizations about word meaning [@xu2007word]. 

We make use of this framework to study pragmatic cue integration across development. To this end, we adapt a method used in perceptual cue integration studies [@ernst2002humans]: we make independent measurements of each cue’s strength and then combine them using the RSA model described above to make independent predictions about conditions in which they either coincide or conflict. Finally, we pre-register these quantitative predictions and test them against new data from adults and children.

We start by replicating previous findings with adults showing that listeners make pragmatic inferences based on non-linguistic properties of utterances in isolation (experiment 1). Then we show that adults make inferences based on common ground information (experiment 2A and 2B). We use data from these experiments as parameters to generate a priori predictions from RSA models about how utterance and common ground information should be integrated. We consider three models that make different assumptions about the integration process: In the *integration model*, the two information sources are integrated with one another. The other two models are lesion models that assume that participants focus on one type of information and disregard the other whenever they are presented together. According to the *no common ground* model, participants focus only on the utterance information and in the *no informativeness* model, only common ground information is considered. We compare predictions from these models to new empirical data from experiments in which utterance and common ground information are manipulated simultaneously (Experiment 3 and 4). 

After successfully validating this approach with adults in study 1, we apply the same model-driven experimental procedure to children (study 2): We first show that they make pragmatic inferences based on utterance and common ground information separately (experiment 5 and 6). Then we generate a priori model predictions and compare them to data from an experiment in which both information sources have to be integrated (experiment 7).

Taken together, this work makes two primary contributions: first, it shows that both adults and children integrate utterance-level and common-ground information flexibly. Second, it uses Bayesian data analysis within the RSA framework to provide a model for understanding the multiple loci for developmental change in complex behaviors like contextual communication.

```{r fig1, include = T, fig.align = "center", fig.cap = "Schematic experimental procedure with screenshots from the adult experiments. In all conditions, at test (bottom), the speaker ambiguously requested an object using a non-word (e.g. “dax”). Participants clicked on the object they thought the speaker referred to. Speech bubbles represent pre-recorded utterances. Informativeness (a) translated to making one object less frequent in context. Common ground (b) was manipulated by making one object preferred by or new to the speaker. Green plus signs represent utterances that expressed preference and red minus signs represent utterances that expressed dispreference (see main text for details). Integration (c) combined informativeness and common ground manipulations. One integration condition is shown here: preference - same speaker - incongruent.", out.width="100%"}
knitr::include_graphics("./figures/fig1.png")
```

# Study 1: Adults

## Participants

Adult participants were recruited via Amazon Mechanical Turk (MTurk) and received payment equivalent to an hourly wage of ~ \$9. Each participant contributed data to only one experiment. Experiment 1 and each manipulation of experiment 2 had *N* = 40 participants. Sample size in experiment 3 was *N* = `r length(unique(adult_ex3_data$id))`. *N* = `r length(unique(adult_ex4_prior_data$id))` participated in the experiments to measure the strong, medium and weak preference and novelty manipulations that went into experiment 4. Finally, experiment 4 had *N* = `r length(unique(adult_ex4_data$id))` participants. Sample sizes in all adult experiments were chosen to yield at least 120 data points per cell. All studies were approved by the Stanford Institutional Review Board (protocol no. 19960).

## Materials

All experimental procedures were pre-registered (see https://osf.io/u7kxe/registrations). Experimental stimuli are freely available in the following online repository: https://github.com/manuelbohn/mcc. All experiments were framed as games in which participants would learn words from animals. They were implemented in HTML/JavaScript as a website. Adults were directed to the website via MTurk and responded by clicking objects. For each animal character, we recorded a set of utterances (one native English speaker per animal) that were used to provide information and make requests. All experiments started with an introduction to the animals and two training trials in which familiar objects were requested (car and ball). Subsequent test trials in each condition were presented in a random order.

## Analytic approach

We preregistered sample sizes, inferential statistical analysis and computational models for all experiments. All deviations from the registered analysis plan are explicitly mentioned. All analyses were run in R [@R-base]. All p-values are based on two sided analysis. Cohen's d (computed via the function `cohensD`) was used as effect size for t-tests. Frequentist logistic GLMMs were fit via the function `glmer` from the package `lme4` [@R-lme4] and had a maximal random effect structure conditional on model convergence. Details about GLMMs including model formulas for each experiment can be found in the Supplementary Material available online.
Probabilistic models and model comparisons were implemented in WebPPL [@dippl] using the R package `rwebppl` [@R-rwebppl]. In experiment 3, 4 and 7, we compared probabilistic models based on Bayes Factors which were calculated from the marginal likelihoods of each model given the data. Details on models, including information about priors for parameter estimation and Markov chain Monte Carlo settings can be found in the Supplementary Material available online. Code to run the models is available in the associated online repository.

## Experiment 1

### Methods

```{r ex1 results}
ex1t <- adult_ex1_data %>%
  group_by(condition, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(condition) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         mean = mean(unlist(correct)),
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         t_lci = t.test(unlist(correct), mu = 0.5)$conf.int[1],
         t_uci = t.test(unlist(correct), mu = 0.5)$conf.int[2],
         p_value = t.test(unlist(correct), mu = 0.5)$p.value,
         d = cohensD(unlist(correct), mu = 0.5)) %>%
  select(condition,mean,df,t_value,t_lci, t_uci,p_value,d)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2),
         t_lci = round(t_lci,2),
         t_uci = round(t_uci,2),
         d = round(d,2))
# model
lm_ex1 <- glmer(correct ~ condition +
              (1 |id), 
              data = adult_ex1_data, 
              family = binomial,control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

#summary(lm_ex1)

ex1_r <- tidy(lm_ex1, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))
```

In experiment 1, participants could learn which object a novel word referred to by assuming that the speaker communicated in an informative way [@frank2014inferring]. The speaker was located between two tables, one with two novel objects, A and B, and the other with only object A (Fig \ref{fig:fig1}a). At test, the speaker turned and pointed to the table with the two objects (A and B) and used a novel word to request one of them. The same utterance was used to make a request in all adult studies ( “Oh cool, there is a [non-word] on the table, how neat, can you give me the [non-word]?”). Participants could infer that the word referred to object B via the counter-factual inferences that, if the (informative) speaker had wanted to refer to object A, they would have pointed to the table with the single object (this being the least ambiguous way to refer to that object). In the control condition, both tables contained both objects and no inference could be made based on the speaker’s behavior. Participants received six trials, three per condition. 

### Results

Participants selected object B above chance in the test condition (mean = `r ex1t%>%filter(condition == "test")%>%pull(mean)`, 95% CI of mean = [`r ex1t%>%filter(condition == "test")%>%pull(t_lci)`; `r ex1t%>%filter(condition == "test")%>%pull(t_uci)`], t(`r ex1t%>%filter(condition == "test")%>%pull(df)`) = `r ex1t%>%filter(condition == "test")%>%pull(t_value)`, *p* `r ex1t%>%filter(condition == "test")%>%pull(p_value)`, d = `r ex1t%>%filter(condition == "test")%>%pull(d)`) and more often compared to the control condition (*$\beta$* = `r ex1_r%>%filter(term == "conditiontest")%>%pull(estimate)`, se = `r ex1_r%>%filter(term == "conditiontest")%>%pull(std.error)`, *p* `r ex1_r%>%filter(term == "conditiontest")%>%pull(p.value)`, see Fig \ref{fig:fig2}). This finding replicates earlier work showing that adult listeners expect speakers to communicate in an informative way.

## Experiment 2

### Methods

```{r ex2-results}
ex2tpref <- adult_ex2_preference_data %>%
  group_by(condition, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(condition) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         mean = mean(unlist(correct)),
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         t_lci = t.test(unlist(correct), mu = 0.5)$conf.int[1],
         t_uci = t.test(unlist(correct), mu = 0.5)$conf.int[2],
         p_value = t.test(unlist(correct), mu = 0.5)$p.value,
         d = cohensD(unlist(correct), mu = 0.5)) %>%
  select(condition,mean,df,t_value,t_lci, t_uci,p_value,d)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2),
         t_lci = round(t_lci,2),
         t_uci = round(t_uci,2),
         d = round(d,2))


# model (maximally converging)
lm_ex2pref <- glmer(correct ~ condition +
              (1|id) + (1|agent), 
              data = adult_ex2_preference_data, 
              family = binomial,control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

#summary(lm_ex2pref)

ex2pref_r <- tidy(lm_ex2pref, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))

ex2tnov <- adult_ex2_novelty_data %>%
  group_by(condition, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(condition) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         mean = mean(unlist(correct)),
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         t_lci = t.test(unlist(correct), mu = 0.5)$conf.int[1],
         t_uci = t.test(unlist(correct), mu = 0.5)$conf.int[2],
         p_value = t.test(unlist(correct), mu = 0.5)$p.value,
         d = cohensD(unlist(correct), mu = 0.5)) %>%
  select(condition,mean,df,t_value,t_lci, t_uci,p_value,d)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2),
         t_lci = round(t_lci,2),
         t_uci = round(t_uci,2),
         d = round(d,2))

# model
lm_ex2nov <- glmer(correct ~ condition +
              (condition|id), 
              data = adult_ex2_novelty_data, 
              family = binomial,control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

#summary(lm_ex2nov)

ex2nov_r <- tidy(lm_ex2nov, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))
```

In experiments 2A and 2B, we tested if participants use common ground information that is specific to a speaker to identify the referent of a novel word [@akhtar1996role; @diesendruck2004two; @saylor2009preschoolers]. In experiment 2A, the speaker expressed a preference for one of two objects (Fig \ref{fig:fig1}b, left). The animal introduced themselves, then turned to one of the tables and expressed either that they liked (“Oh wow, I really like that one”) or disliked (“Oh bleh, I really don’t like that one”) the object before turning to the other side and expressing the respective other attitude. Next the animal disappeared and, after a short pause, either the same or a different animal returned and requested an object while facing straight ahead. Participants could use the speakers preference to identify the referent when the same speaker returned but not when a different speaker appeared whose preferences were unknown.

In experiment 2B, common ground information came in the form of novelty (Fig \ref{fig:fig1}b, right). The animal turned to one of the sides and commented either on the presence (“Aha, look at that”) or the absence (“Hm…, nothing there”) of an object before turning to the other side and commenting in a complementary way. Later, a second object appeared on the previously empty table. Then the speaker used a novel word to request one of the objects. The referent of the novel word could be identified by assuming that the speaker uses it to refer to the object that is new to them. This inference was not licensed when a different speaker returned to whom both objects were equally new. For both novelty and preference, participants received six trials, three with the same and three with the different speaker.

### Results

In experiment 2A, participants selected the preferred object above chance (mean = `r ex2tpref%>%filter(condition == "same_speaker")%>%pull(mean)`, 95% CI of mean = [`r ex2tpref%>%filter(condition == "same_speaker")%>%pull(t_lci)`; `r ex2tpref%>%filter(condition == "same_speaker")%>%pull(t_uci)`], t(`r ex2tpref%>%filter(condition == "same_speaker")%>%pull(df)`) = `r ex2tpref%>%filter(condition == "same_speaker")%>%pull(t_value)`, *p* `r ex2tpref%>%filter(condition == "same_speaker")%>%pull(p_value)`, d = `r ex2tpref%>%filter(condition == "same_speaker")%>%pull(d)`)  and more so than in the speaker change control condition ($\beta$ = `r ex2pref_r%>%filter(term == "conditionsame_speaker")%>%pull(estimate)`, se = `r ex2pref_r%>%filter(term == "conditionsame_speaker")%>%pull(std.error)`, *p* `r ex2pref_r%>%filter(term == "conditionsame_speaker")%>%pull(p.value)`).

In experiment 2B, participants selected the novel object above chance (mean = `r ex2tnov%>%filter(condition == "same_speaker")%>%pull(mean)`, 95% CI of mean = [`r ex2tnov%>%filter(condition == "same_speaker")%>%pull(t_lci)`; `r ex2tnov%>%filter(condition == "same_speaker")%>%pull(t_uci)`], t(`r ex2tnov%>%filter(condition == "same_speaker")%>%pull(df)`) = `r ex2tnov%>%filter(condition == "same_speaker")%>%pull(t_value)`, *p* `r ex2tnov%>%filter(condition == "same_speaker")%>%pull(p_value)`, d = `r ex2tnov%>%filter(condition == "same_speaker")%>%pull(d)`) when the same speaker made the request and more often compared to when a different speaker made the request ($\beta$ = `r ex2nov_r%>%filter(term == "conditionsame_speaker")%>%pull(estimate)`, se = `r ex2nov_r%>%filter(term == "conditionsame_speaker")%>%pull(std.error)`, *p* `r ex2nov_r%>%filter(term == "conditionsame_speaker")%>%pull(p.value)`, see Fig \ref{fig:fig2}).

```{r fig2, include = T, fig.align = "center", fig.cap = "Results from experiments 1, 2A, and 2B for adults. For preference and novelty, control refers to a different speaker (see Fig 1b). Transparent dots show data from individual participants, diamonds represent condition means, error bars are 95\\% CIs. Dashed line indicates performance expected by chance.", out.width="60%"}
knitr::include_graphics("./figures/fig2.png")
```

## Modelling information integration

Experiments 1 and 2 confirmed that adults make pragmatic inferences based on information provided by the utterance as well as by common ground and provided quantitative estimates of the strength of these inferences for use in our model. We modeled the integration of utterance informativity and common ground as a process of socially-guided probabilistic inference, using the results of experiments 1 and 2 to inform key parameters of a computational model. The Rational Speech Act (RSA) model architecture introduced by Frank and Goodman [-@frank2012predicting] encodes conversational reasoning through the perspective of a listener (“he” pronoun) who is trying to decide on the intended meaning of the utterance he heard from the speaker (“she” pronoun). The basic idea is that the listener combines his uncertainty about the speaker’s intended meaning - a prior distribution over referents P(r) - with his generative model of how the utterance was produced: a speaker trying to convey information to him. To adapt this model to the word learning context, we enrich this basic architecture with a mechanism for expressing uncertainty about the meanings of words (lexical uncertainty) - a prior distribution over lexica P(L) [@bergen2016pragmatic]. 

$$P_{L}(r, \mathcal{L}|u)\propto P_{S}(u|r, \mathcal{L}) \cdot P( \mathcal{L}) \cdot P(r)$$

In the above equation, the listener is trying to jointly resolve the speaker’s intended referent r and the meaning of words (thus learning the lexicon $\mathcal{L}$). He does this by imagining what a rational speaker would say, given the referent they are trying to communicate and a lexicon. The speaker is an approximately rational Bayesian actor (with degree of rationality alpha), who produces utterances as a function of their informativity. The space of utterances the speaker could produce depends upon the lexicon $P(u|\mathcal{L})$; simply put, the speaker labels objects with the true labels under a given lexicon L (see Supplementary Material available online for details):

$$P_{S}(u|r,\mathcal{L})\propto Informativity(u;r)^\alpha \cdot P(u|\mathcal{L})$$

The informativity of an utterance for a referent is taken to be the probability with which a naive listener, who only interprets utterances according to their literal semantics, would select a particular referent given an utterance.

$$Informativity(u; r) = P(r|u) \propto P(r) \cdot \mathcal{L}_{point}$$

The speaker’s possible utterances are pairs of linguistic and non-linguistic signals, namely labels and points. Because the listener does not know the lexicon, the informativity of an utterance comes from the speaker’s point, the meaning of which is encoded in $\mathcal{L}_{point}$ and is simply a truth-function checking whether or not the referent is at the location picked out by the speaker’s point. Though the speaker makes their communicative decision assuming the listener does not know the meaning of the labels, we assume that in addition to a point, the speaker produces a label consistent with their own lexicon $\mathcal{L}$, described by $P(u|\mathcal{L})$ (see Supplementary Material available online for modeling details).

This computational model provides a natural avenue to formalize quantitatively how informativeness and common ground trade-off during word learning. As mentioned above, the common ground shared between speaker and listener plays the role of the listener’s prior distribution over meanings, or types of referents, that the speaker might be referring to and which we posit depends on prior interactions around the referents in the present context (e.g., preference or novelty; experiment 2A and B). We use the results from experiment 2 to specify this distribution. The in-the-moment, contextual informativeness of the utterance is captured in the likelihood term, whose value depends on the rationality parameter $\alpha$. Assumptions about rationality may change depending on context and we therefore used the data from experiment 1 to specify $\alpha$ (see Supplementary Material available online for details about these parameters). 	

The model generates predictions for situations in which utterance and common ground expectations are jointly manipulated (Fig \ref{fig:fig1}c - see Supplementary Material available online for additional details and a worked example of how predictions were generated). In addition to the parameters fit to the data from previous experiments, we include an additional noise parameter, which can be thought of as reflecting the cost that comes with handling and integrating multiple information sources. Technically it estimates the proportion of responses better explained by a process of random guessing than by pragmatics; we estimate this parameter from the observed data (experiment 3). Including the noise parameter greatly improved the model fit to the data (see Supplementary Material available online for details). We did not pre-register the inclusion of a noise parameter for experiment 3 but did so for all subsequent experiments.

## Experiment 3

### Methods

```{r ex3 GLMM, cache=F}
# model 
lm_ex3 <- glmer(correct_inf ~ common_ground_manipulation*speaker*alignment + (alignment|id), 
              data = adult_ex3_data, family = binomial, 
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

#summary(lm_ex3)

lm_ex3_r <- tidy(lm_ex3, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))

```

```{r model comparison ex3}
ex3_bf <- bind_rows(
  readRDS("../../stats/saves/ex3_prior_only_model_noise_loglike.rds") %>% mutate(model = "prior_only_noise"),
  readRDS("../../stats/saves/ex3_pragm_model_noise_loglike.rds") %>% mutate(model = "pragmatic_noise"),
  readRDS("../../stats/saves/ex3_flat_prior_model_noise_loglike.rds") %>% mutate(model = "flat_prior_noise")
)%>%
  group_by(model)%>%
  summarize(logP = logSumExp(value))%>%
  bind_rows(readRDS("../../stats/saves/ex3_pragm_model_loglike.rds") %>% mutate(model = "pragmatic_parameter_free"))%>%
  spread(model, logP) %>%
  mutate("pragmatic_noise_pragmatic_parameter_free" = exp(pragmatic_noise - pragmatic_parameter_free),
         "pragmatic_noise_prior_only_noise" = exp(pragmatic_noise - prior_only_noise),
         "pragmatic_noise_flat_prior_noise" = exp(pragmatic_noise - flat_prior_noise),
         "prior_only_noise_flat_prior_noise" = exp(prior_only_noise - flat_prior_noise)) %>%
  select(-pragmatic_parameter_free,-pragmatic_noise, -flat_prior_noise, -prior_only_noise)


ex3_pragm_model_noise_param <- readRDS("../../stats/saves/ex3_pragm_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))%>%
  round(digits = 2)

ex3_flat_prior_model_noise_param <- readRDS("../../stats/saves/ex3_flat_prior_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))%>%
  round(digits = 2)

ex3_prior_only_model_noise_param <- readRDS("../../stats/saves/ex3_prior_only_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))%>%
  round(digits = 2)
```

In experiment 3, we combined the procedures of experiment 1 and 2A or 2B. The test setup was identical to experiment 1, however, before making a request, the speaker interacted with the objects so that some of them were preferred by or new to them (Fig \ref{fig:fig1}c). This combination resulted in two ways in which the two information sources could be aligned with one another. In the congruent condition, the object that was the more informative referent was also the one that was preferred by or new to the speaker. In the incongruent condition, the other object was the one that was preferred by or new to the speaker. Taken together, there were 2 (novelty or preference) x 2 (same or different speaker) x 2 (congruent or incongruent) = 8 conditions in experiment 3. For each of these eight conditions, we generated model predictions using the modelling framework introduced above. The test hypothesis about how information is integrated we compared the three models introduced in the introduction: The *integration model* in which both information sources are flexibly combined, the *no common ground model* that focused only on utterance-level information and the the *no informativeness model* that focused only on common ground information.

Participants completed eight trials for one of the common ground manipulations with two trials per condition (same/different speaker x congruent/incongruent). Conditions were presented in a random order. We discuss and visualize the results as the proportion with which participants chose the more informative object (i.e., the object that would be the more informative referent when only utterance information is considered). 

### Results

As a first step, we used a GLMM to test whether participants were sensitive to the different ways in which information could be aligned. We found that participants distinguished between congruent and incongruent trials when the speaker remained the same (model term: `alignment x speaker`; $\beta$ = `r lm_ex3_r%>%filter(term == "speakersame_speaker:alignmentincongruent")%>%pull(estimate)`, se = `r lm_ex3_r%>%filter(term == "speakersame_speaker:alignmentincongruent")%>%pull(std.error)`, *p* `r lm_ex3_r%>%filter(term == "speakersame_speaker:alignmentincongruent")%>%pull(p.value)`). Thus, participants were sensitive to the different combinations of manipulations.

```{r fig3, include = T, fig.align = "center", fig.cap = "Results from experiment 3 and 4 for adults. Data and model predictions by condition for experiment 3 (a). Transparent dots show data from individual participants, diamonds represent condition means. Correlation between model predictions and data in Experiment 3 (b), between data in Experiment 3 and the direct replication in experiment 4 (c) and between model predictions and data in experiment 4 (d). Coefficients and p-values are based on Pearson correlation statistics. Error bars represent 95\\% HDIs." , out.width="100%"}

knitr::include_graphics("./figures/fig3.png")
```

As a second step, we compared the model predictions to the data. Participants’ average responses were highly correlated with the  predictions from the *integration model* in each condition (Fig \ref{fig:fig3}b). When comparing model, we found that model fit was considerably better for the* integration model* compared to the *no common ground model* (Bayes Factor (BF) = `r format(ex3_bf%>%pull(pragmatic_noise_flat_prior_noise), digits = 2)`) or the *no informativeness model* (BF = `r format(ex3_bf%>%pull(pragmatic_noise_prior_only_noise), digits = 2)`), suggesting that participants considered and integrated both sources of information. 

Finally, we examined the noise parameter for each model. The estimated proportion of random responses according to the *integration model* was `r ex3_pragm_model_noise_param%>%pull(mean)` (95% Highest Density Interval (HDI): `r ex3_pragm_model_noise_param%>%pull(ci_lower)` - `r ex3_pragm_model_noise_param%>%pull(ci_upper)`). This parameter was substantially lower for the *integration model* compared to the alternative models (*no common ground model*: `r ex3_flat_prior_model_noise_param%>%pull(mean)` [`r ex3_flat_prior_model_noise_param%>%pull(ci_lower)` - `r ex3_flat_prior_model_noise_param%>%pull(ci_upper)`]; *no informativeness model*: `r ex3_prior_only_model_noise_param%>%pull(mean)` [`r ex3_prior_only_model_noise_param%>%pull(ci_lower)` - `r ex3_prior_only_model_noise_param%>%pull(ci_upper)`]), lending additional support to the conclusion that the *integration model* better captured the behavioral data. Rather than explaining systematic structure in the data, the alternative models achieved their best fit only by assuming a very high level of noise.

## Experiment 4

### Methods

```{r model comparison ex4}
ex4_bf <- bind_rows(
  readRDS("../../stats/saves/ex4_prior_only_model_noise_loglike.rds") %>% mutate(model = "prior_only_noise"),
  readRDS("../../stats/saves/ex4_pragm_model_noise_loglike.rds") %>% mutate(model = "pragmatic_noise"),
  readRDS("../../stats/saves/ex4_flat_prior_model_noise_loglike.rds") %>% mutate(model = "flat_prior_noise")
)%>%
  group_by(model)%>%
  summarize(logP = logSumExp(value))%>%
  spread(model, logP) %>%
  mutate("pragmatic_noise_prior_only_noise" = exp(pragmatic_noise - prior_only_noise),
         "pragmatic_noise_flat_prior_noise" = exp(pragmatic_noise - flat_prior_noise),
         "prior_only_noise_flat_prior_noise" = exp(prior_only_noise - flat_prior_noise)) %>%
  select(-pragmatic_noise, -flat_prior_noise, -prior_only_noise)

ex4_pragm_model_noise_param <- readRDS("../../stats/saves/ex4_pragm_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))%>%
  round(digits = 2)

ex4_flat_prior_model_noise_param <- readRDS("../../stats/saves/ex4_flat_prior_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))%>%
  round(digits = 2)

ex4_prior_only_model_noise_param <- readRDS("../../stats/saves/ex4_prior_only_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))%>%
  round(digits = 2)


```

To test if the *integration model* makes accurate predictions for different combinations, we first replicated and then extended the results of experiment 3 to a broader range of experimental conditions. Specifically, we manipulated the strength of the common ground information (3 levels - strong, medium and weak - for preference and 2 levels - strong and medium - for novelty) by changing the way the speaker interacted with the objects prior to the request. The procedural details and statistical analysis for these these manipulations are described in the Supplementary Material available online. For experiment 4, we paired each level of prior strength manipulation with the informativeness inference in the same way as in experiment 3. This resulted in a total of 20 conditions, for which we generated a priori model predictions in the same way as in experiment 3. The strong prior manipulation in experiment 4 was a direct replication of experiment 3 (see Fig \ref{fig:fig3}c). Each participant was randomly assigned to a common ground manipulation and a level of prior strength and completed eight trials in total, two in each unique condition in that combination. 

### Results

The direct replication of experiment 3 within experiment 4 showed a very close correspondence between the two rounds of data collection (see Fig \ref{fig:fig3}c). GLMM results for experiment 4 can be found in the Supplementary Material available online. Here we focus on the analysis based on the probabilistic models. Model predictions from the *integration model* were again highly correlated with the average response in each condition (see Fig \ref{fig:fig3}d). We evaluated model fit for the same models as in experiment 3 and found again that the *integration model* fit the data much better compared to the *no common ground* (BF = `r format(ex4_bf%>%pull(pragmatic_noise_flat_prior_noise), digits = 2)`) or the *no informativeness model* (BF = `r format(ex4_bf%>%pull(pragmatic_noise_prior_only_noise), digits = 2)`). The inferred level of noise based on the data for the *integration model* was `r ex4_pragm_model_noise_param%>%pull(mean)` (95% HDI: `r ex4_pragm_model_noise_param%>%pull(ci_lower)` - `r ex4_pragm_model_noise_param%>%pull(ci_upper)`), which was similar to experiment 3 and again lower compared to the alternative models (*no common ground model*: `r ex4_flat_prior_model_noise_param%>%pull(mean)` [`r ex4_flat_prior_model_noise_param%>%pull(ci_lower)` - `r ex4_flat_prior_model_noise_param%>%pull(ci_upper)`]; *no informativeness model*: `r ex4_prior_only_model_noise_param%>%pull(mean)` [`r ex4_prior_only_model_noise_param%>%pull(ci_lower)` - `r ex4_prior_only_model_noise_param%>%pull(ci_upper)`]).

# Study 2: Children

The previous section showed that competent language users flexibly integrate information during pragmatic word learning. Do children make use of multiple information sources during word learning as well? When does this integration emerge developmentally? While many verbal theories of language learning imply that this integration takes place, the actual process has neither been described nor tested in detail. Here we provide an explanation in the form of our *integration model* and test if it is able to capture children’s word learning. Embedded in the assumptions of the model is the idea that developmental change is change in the strength of the individual inferences, leading to a change in the strength of the integrated inference. As a starting point, our model assumes developmental continuity in the integration process itself [@bohn2019pervasive], though this assumption could be called into question by a poor model fit. The study for children followed the same general pattern as the one for adults. We generated model predictions for how information should be integrated by first measuring children’s ability to use utterance (informativeness) and common ground (preference) information in isolation when making pragmatic inferences. We then adapted our model to study developmental change: We sampled children continuously between 3.0 and 5.0 years of age – a time in which children have been found to make the kind of pragmatic inferences we studied here [@bohn2019pervasive; @frank2014inferring] - and generated model predictions for the average developmental trajectory in each condition.

## Participants

Children were recruited from the floor of the Children’s Discovery Museum in San Jose, California, USA. Parents gave informed consent and provided demographic information. Each child contributed data to only one experiment. We collected data from a total of 243 children between 3.0 and 5.0 years of age. We excluded 15 children due to less than 75% of reported exposure to English, five because they responded incorrectly on 2/2 training trials, three because of equipment malfunction, and two because they quit before half of the test trials were completed. The final sample size in each experiment was as follows: *N* = `r length(unique(child_ex1_data$id))` (41 girls, mean age = `r round(mean(child_ex1_data$age_num),2)`) in experiment 5,  *N* = `r length(unique(child_ex2_data$id))` (28 girls, mean age = `r round(mean(child_ex2_data$age_num),2)`) in experiment 6 and *N* = `r length(unique(child_ex3_data$id))` (54 girls, mean age = `r round(mean(child_ex3_data$age_num),2)`) in experiment 7. For experiment 5 and 6, we also tested two-year-olds but did not find sufficient evidence that they use utterance and/or common ground information in the tasks we used to justify investigating their ability to integrate the two. Sample sizes in all experiments were chosen to yield at least 80 data points in each cell for each age group.

## Materials

Experiments were implemented in the same general way as for adults. Children were guided through the games by an experimenter and responded by touching objects on the screen of an iPad tablet [@frank2016using].

## Experiment 5

### Methods

```{r child ex1 and 2 results}
#experiment 1
child_ex1t <- child_ex1_data %>%
  group_by(age_bin, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(age_bin) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         mean = mean(unlist(correct)),
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         t_lci = t.test(unlist(correct), mu = 0.5)$conf.int[1],
         t_uci = t.test(unlist(correct), mu = 0.5)$conf.int[2],
         p_value = t.test(unlist(correct), mu = 0.5)$p.value,
         d = cohensD(unlist(correct), mu = 0.5)) %>%
  select(age_bin,mean,df,t_value,t_lci, t_uci,p_value,d)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2),
         t_lci = round(t_lci,2),
         t_uci = round(t_uci,2),
         d = round(d,2))


child_ex1_lm_data <- child_ex1_data%>%
  mutate(age_num = scale(age_num, center = TRUE, scale = TRUE))

# GLMM
child_ex1_lm <- glmer(correct ~ age_num + (1 | id), 
              data = child_ex1_lm_data, family = binomial, 
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

#summary(child_ex1_lm)

child_lm_ex1_r <- tidy(child_ex1_lm, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))

# experiment 2
child_ex2t <- child_ex2_data %>%
  group_by(condition,age_bin, id) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = list(correct)) %>%
  group_by(condition,age_bin) %>%
  mutate(df= t.test(unlist(correct), mu = 0.5)$parameter,
         mean = mean(unlist(correct)),
         t_value = t.test(unlist(correct), mu = 0.5)$statistic,
         t_lci = t.test(unlist(correct), mu = 0.5)$conf.int[1],
         t_uci = t.test(unlist(correct), mu = 0.5)$conf.int[2],
         p_value = t.test(unlist(correct), mu = 0.5)$p.value,
         d = cohensD(unlist(correct), mu = 0.5)) %>%
  select(condition,age_bin,mean,df,t_value,t_lci, t_uci,p_value,d)%>%
  mutate(p_value = ifelse(p_value<.001,"< .001",as.character(paste("=",substr(round(p_value,3),2,5),sep=" "))),
         t_value = round(t_value,2),
         t_lci = round(t_lci,2),
         t_uci = round(t_uci,2),
         d = round(d,2))

# GLMM

child_ex2_lm_data <- child_ex2_data%>%
  mutate(age_num = scale(age_num, center = TRUE, scale = TRUE))

child_ex2_lm <- glmer(correct ~ age_num*condition + (condition | id) + (1 | agent), 
              data = child_ex2_lm_data, family = binomial, 
              control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

summary(child_ex2_lm)

child_lm_ex2_r <- tidy(child_ex2_lm, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))

```

Experiment 5 for children was modeled after @frank2014inferring. Instead of on tables, objects were presented as hanging in trees (to facilitate showing points to distinct locations). After introducing themselves, the animal turned to the tree with two objects and said: “This is a tree with a [non-word], how neat, a tree with a [non-word]”). Next, the trees and the objects in them disappeared and new trees replaced them. The two objects from the tree the animal turned to previously were now spread across the two trees (one object per tree, position counterbalanced). While facing straight, the animal first said “Here are some more trees” and then asked the child to pick the tree with the object that corresponded to the novel word (“Which of these trees has a [non-word]?”). Children received six trials in a single test condition. 

### Results

To compare children’s performance to chance level, we binned age by year. Four-year-olds selected the more informative object (i.e. the object that was unique to the location the speaker turned to) above chance (mean = `r child_ex1t%>%filter(age_bin =="4")%>%pull(mean)`, 95% CI of mean = [`r child_ex1t%>%filter(age_bin =="4")%>%pull(t_lci)`; `r child_ex1t%>%filter(age_bin =="4")%>%pull(t_uci)`], t(`r child_ex1t%>%filter(age_bin =="4")%>%pull(df)`) = `r child_ex1t%>%filter(age_bin =="4")%>%pull(t_value)`, *p* `r child_ex1t%>%filter(age_bin =="4")%>%pull(p_value)`, d = `r child_ex1t%>%filter(age_bin =="4")%>%pull(d)`). Three-year-olds, on the other hand, did not (mean = `r child_ex1t%>%filter(age_bin =="3")%>%pull(mean)`, 95% CI of mean = [`r child_ex1t%>%filter(age_bin =="3")%>%pull(t_lci)`; `r child_ex1t%>%filter(age_bin =="3")%>%pull(t_uci)`], t(`r child_ex1t%>%filter(age_bin =="3")%>%pull(df)`) = `r child_ex1t%>%filter(age_bin =="3")%>%pull(t_value)`, *p* `r child_ex1t%>%filter(age_bin =="3")%>%pull(p_value)`, d = `r child_ex1t%>%filter(age_bin =="3")%>%pull(d)`). Consequently, when we fit a GLMM to the data with age as a continuous predictor, performance increased with age ($\beta$ = `r child_lm_ex1_r%>%filter(term == "age_num")%>%pull(estimate)`, se = `r child_lm_ex1_r%>%filter(term == "age_num")%>%pull(std.error)`, *p* `r child_lm_ex1_r%>%filter(term == "age_num")%>%pull(p.value)`, see Fig \ref{fig:fig4}). Thus, children’s ability to use utterance information in a word learning context increased with age.

```{r fig4, include = T, fig.align = "center", fig.cap = "Results from experiment 5 and 6 for children. For preference, control refers to to the different speaker condition (see Fig. 1B). Transparent dots show data from individual participants, regression lines show fitted linear models with 95\\% CIs. Dashed line indicates performance expected by chance.", out.width="60%"}
knitr::include_graphics("./figures/fig4.png")
```

## Experiment 6

### Methods

In experiment 6, we assessed whether children use common ground information to identify the referent of a novel word. We tested children only with the preference manipulation\footnote{We initially tested children with the novelty as well as the preference manipulation. We found that children made the basic inference in that they selected the object that was preferred by or new to the speaker, but found little evidence that children distinguished between requests made by the same speaker or a different speaker in the case of novelty. This finding contrasts with earlier work (Diesendruck et al., 2004). However, since our focus was on how children integrate informativeness and common ground, we did not follow up on this finding but dropped the novelty manipulation and focused on preference for the remainder of the study}. The procedure for children was identical to the preference manipulation for adults. Children received eight trials, four with the same and four with a different speaker.

### Results

Four-year-olds selected the preferred object above chance when the same speaker made the request (mean = `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="4")%>%pull(mean)`, 95% CI of mean = [`r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="4")%>%pull(t_lci)`; `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="4")%>%pull(t_uci)`], t(`r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="4")%>%pull(df)`) = `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="4")%>%pull(t_value)`, *p* `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="4")%>%pull(p_value)`, d = `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="4")%>%pull(d)`), whereas three-year-olds did not (mean = `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="3")%>%pull(mean)`, 95% CI of mean = [`r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="3")%>%pull(t_lci)`; `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="3")%>%pull(t_uci)`], t(`r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="3")%>%pull(df)`) = `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="3")%>%pull(t_value)`, *p* `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="3")%>%pull(p_value)`, d = `r child_ex2t%>%filter(condition == "same_speaker" ,age_bin =="3")%>%pull(d)`). However, when we fit a GLMM to the data with age as a continuous predictor, we found an effect of speaker identity ($\beta$ = `r child_lm_ex2_r%>%filter(term == "conditionsame_speaker")%>%pull(estimate)`, se = `r child_lm_ex2_r%>%filter(term == "conditionsame_speaker")%>%pull(std.error)`, *p* `r child_lm_ex2_r%>%filter(term == "conditionsame_speaker")%>%pull(p.value)`) but no effect of age ($\beta$ = `r child_lm_ex2_r%>%filter(term == "age_num")%>%pull(estimate)`, se = `r child_lm_ex2_r%>%filter(term == "age_num")%>%pull(std.error)`, *p* `r child_lm_ex2_r%>%filter(term == "age_num")%>%pull(p.value)`) or interaction between speaker identity and age ($\beta$ = `r child_lm_ex2_r%>%filter(term == "age_num:conditionsame_speaker")%>%pull(estimate)`, se = `r child_lm_ex2_r%>%filter(term == "age_num:conditionsame_speaker")%>%pull(std.error)`, *p* `r child_lm_ex2_r%>%filter(term == "age_num:conditionsame_speaker")%>%pull(p.value)`, see Fig \ref{fig:fig4}). Thus, children across the age range used common ground information to infer the referent of a novel word.

## Modelling information integration in children

Model predictions for children were generated using the same model described above for adults. However, to incorporate developmental change in the model, we allowed the rationality parameter $\alpha$ and the prior distribution over objects to change with age. That is, instead of a single value, we inferred the intercept and slope for each parameter that best described the developmental trajectory in the data of experiment 5 and 6. These parameter settings were then used to generate age sensitive model predictions in 2 (same or different speaker) x 2 (congruent or incongruent) = 4 conditions. As for adults, all models included a noise parameter, which was estimated based on the data of experiment 7.

## Experiment 7

### Methods

```{r child ex3 GLMM, cache = F}
child_ex3_lm_data <- child_ex3_data%>%
  mutate(age_num = scale(age_num, center = TRUE, scale = TRUE))

child_ex3_lm <- glmer(correct_inf ~ age_num*speaker*alignment 
      + (speaker+alignment | id), 
      family = "binomial",
      data = child_ex3_lm_data,
      control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

summary(child_ex3_lm)

child_lm_ex3_r <- tidy(child_ex3_lm, effects = "fixed")%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2))
```

In experiment 7, we combined the procedures of experiment 5 and 6 and collected new data from children between 3.0 and 5.0 years of age in each of the four conditions (Fig \ref{fig:fig1}c). We again inserted the preference manipulation into the setup of experiment 5. After greeting the child, the animal turned to one of the trees, pointed to an object (object was temporarily enlarged and moved closer to the animal) and expressed liking or disliking. Then the animal turned to the other tree and expressed the other attitude for the other kind of object. Next, the animal disappeared and either the same or a different animal returned. The rest of the trial was identical to the request phase of experiment 5. Children received eight trials, two per condition (same/different speaker x congruent/incongruent) in a randomized order.

### Results

As a first step, we used a GLMM to test whether children were sensitive to the different ways in which information could be aligned. Children’s propensity to differentiate between congruent and incongruent trials for the same or a different speaker increased with age (model term: `age x alignment x speaker`; $\beta$ = `r child_lm_ex3_r%>%filter(term == "age_num:speakersame_speaker:alignmentincongruent")%>%pull(estimate)`, se = `r child_lm_ex3_r%>%filter(term == "age_num:speakersame_speaker:alignmentincongruent")%>%pull(std.error)`, *p* `r child_lm_ex3_r%>%filter(term == "age_num:speakersame_speaker:alignmentincongruent")%>%pull(p.value)`).

```{r child model comparison}

child_ex3_bf <- readRDS("../../stats/saves/child_ex3_model_comparison.rds") %>%
  group_by(parameter)%>%
  spread(model, logP) %>%
  mutate("pragmatic_flat_prior" = exp(pragmatic - flat_prior),
         "pragmatic_prior_only" = exp(pragmatic - prior_only),
         "flat_prior_prior_only" = exp(flat_prior - prior_only)) %>%
  select(-pragmatic,-flat_prior, -prior_only)


child_ex3_pragm_model_noise_param <- readRDS("../../stats/saves/child_ex3_pragm_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))

child_ex3_flat_prior_model_noise_param <- readRDS("../../stats/saves/child_ex3_flat_prior_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))

child_ex3_prior_only_model_noise_param <- readRDS("../../stats/saves/child_ex3_prior_only_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))

```

Analyses comparing the model predictions from the probabilistic models to the data suggest that children flexibly integrate both common ground and informativity information. Furthermore, this integration process is accurately captured by the *integration model* at least for four-year-olds. For the correlational analysis, we binned model predictions and data by year. There was a substantial correlation between the predicted and measured average response for four-year-olds, but less so for three-year-olds (Fig \ref{fig:fig5}b). One of the reasons for the latter was the low variation between conditions. For the model comparison, we treated age continuously. As with adults, we found a much better model fit for the *integration model* compared to the *no common ground* (BF = `r format(child_ex3_bf%>%filter (parameter=="noise")%>%pull(pragmatic_flat_prior), digits = 2)`) or the *no informativeness model* (BF = `r format(child_ex3_bf%>%filter (parameter=="noise")%>%pull(pragmatic_prior_only), digits = 2)`). 

The inferred level of noise based on the data for the integration model was `r child_ex3_pragm_model_noise_param%>%pull(mean)` (95% HDI: `r child_ex3_pragm_model_noise_param%>%pull(ci_lower)` - `r child_ex3_pragm_model_noise_param%>%pull(ci_upper)`), which was lower compared to the alternative models considered (*no common ground model*: `r child_ex3_flat_prior_model_noise_param%>%pull(mean)` [`r child_ex3_flat_prior_model_noise_param%>%pull(ci_lower)` - `r child_ex3_flat_prior_model_noise_param%>%pull(ci_upper)`]; *no informativeness model*: `r child_ex3_prior_only_model_noise_param%>%pull(mean)` [`r child_ex3_prior_only_model_noise_param%>%pull(ci_lower)` - `r child_ex3_prior_only_model_noise_param%>%pull(ci_upper)`]) but numerically higher than that of adults. 

The high level of inferred noise moved the model predictions for children in all conditions close to chance level. We therefore compared two additional sets of models with different parameterizations of the noise parameter that emphasized differences between conditions in the model predictions more (see Supplementary Material available online and Fig \ref{fig:fig5}a). This analysis was not pre-registered. Parameter free models did not include a noise parameter and developmental noise models allowed the noise parameter to change with age. In each case, the *integration model* provided a better fit compared to the alternative models (*no common ground*: parameter free BF = `r format(child_ex3_bf%>%filter (parameter=="parameter free")%>%pull(pragmatic_flat_prior), digits = 2)`, developmental noise BF = `r format(child_ex3_bf%>%filter (parameter=="developmental noise")%>%pull(pragmatic_flat_prior), digits = 2)`; *no informativeness*: parameter free BF = `r format(child_ex3_bf%>%filter (parameter=="parameter free")%>%pull(pragmatic_prior_only), digits = 2)`, developmental noise BF = `r format(child_ex3_bf%>%filter (parameter=="developmental noise")%>%pull(pragmatic_prior_only), digits = 2)`). The developmental noise parameter for the integration model decreased with age, suggesting that older children behaved more in line with model predictions compared to younger children (see Fig. S13 in Supplementary Material available online). 

```{r fig5, include = T, fig.align = "center", fig.cap = "Results from experiment 7 for children. Model predictions and data across age in the four conditions (a). Transparent black dots show data from individual participants and black lines show conditional means of the data with 95\\% CI. Black diamonds show the mean of the data for age bins by year and error bars show 95\\% CIs. Correlation between model predictions (with noise parameter) and condition means binned by year (b). Coefficients and p-values are based on Pearson correlation statistic. Error bars and shaded regions represent 95\\% HDIs. For 4-year-olds, two conditions yielded the same data means and model predicitons and are thus plotted on top of each other.", out.width = "80%"}
knitr::include_graphics("./figures/fig5.png")
```

# Discussion

Integrating multiple sources of information is an integral part of human communication [@tanenhaus1995integration]. To infer the intended meaning of an utterance, listeners must combine their knowledge of communicative conventions (semantics and syntax) with social expectations about their interlocutor. This integration is especially vital in early language learning, and the different varieties of pragmatic information are among the most important sources [@bohn2019pervasive]. But how are pragmatic cues integrated during word learning? Here we used a Bayesian cognitive model to formalize this integration process. We studied how utterance-level (Gricean) expectations about informative communication are integrated with common ground information. Adults’ and children’s learning was best predicted by a model in which both sources of information traded-off flexibly. Alternative models that considered only one source of information made substantially worse predictions.

All of the models we compared here integrated some explicit structure, rather than (for example) simply weighing information sources by some ratio. We made this decision because we wanted to make predictions within a framework in which the models were models of the task, rather than simply models of the data. That is, inferences are not computed separately by the modeler and specified as inputs to a regression model, but instead are the results of an integrated process that operates over a (schematic) representation of the experimental stimuli. Further, our models are variants derived from the broader RSA framework, which has been integrated into larger systems for language learning in context [@wang2016learning; @cohn2018pragmatically; @monroe2017colors].

How is information integrated in this context in this context? The *integration model* assumes that the informativeness of an utterance depends on the common ground shared between interlocutors. That is, the listener assumes that the speaker takes the common ground shared between the speaker and the (naive) listener as a starting point when computing the effect of each utterance. As a consequence, when prior interactions strongly implicate one object as the more likely referent (for example in the preference - same speaker conditions in experiment 3, 4 and 7), the speaker reasons that this object will be the inferred referent of any semantically plausible utterance, even when the same utterance would point to a different object in the absence of common ground. Taken together, our model advances classic theories on pragmatic language comprehension [@grice1991studies; @sperber2001relevance] and learning [@brunder1983child; @tomasello2009constructing] by providing an explicit and formal description of the integration process, thereby offering an answer to the question of *how* information may be integrated during pragmatic word learning. Predictions generated based on this process accurately captured adults' inferences across a wide range of conditions.   

The *integration model* predicted information integration in four-year-olds. However, the model did not successfully describe three-year-olds’ inferences; thus, it is possible that they were not able to integrate information sources. But our findings are also consistent with a simpler explanation, namely that the overall weaker responses we observed in the independent measurement experiments (experiments 5 and 6), combined with some noise in responding, led the younger children to appear relatively random in their responses. As a consequence, there was not much variation in three-year-old's responses for the model to explain. 

We did not model the social-cognitive processes that specify the probability of an object being the referent given common ground - we simply measured it empirically. As a consequence, our approach treats common ground as equivalent to more basic manipulations of contextual salience [e.g. in @frank2012predicting]. Thus, our model would not differentiate between a situation in which an object would be salient because it has been the focus of an interaction and one in which it would be more salient because it was big or colorful. A starting point for explicitly modeling common ground would be the work by Heller and colleagues [@heller2016perspective; @mozuraitis2018modeling]. In their work, they focus on how listeners identify the referent of ambiguous referring expressions. Their probabilistic model simultaneously considers the (differing) perspectives of both interlocutors and trades off between them. In principle, the model of Heller and colleagues [-@heller2016perspective] and the *integration model* could easily be combined with one another. However, because common ground in our study was established by a shared interaction and not by the online convergence of different perspectives, it would require a different experimental design to test the combined model.

The primary source of developmental change in our model is age related changes in the propensity to make the individual inferences. As they get older, children expect speakers to be more informative and to be more likely to follow common ground, but the process by which the two information sources are integrated at any given age is assumed to be the same. Other developmental models are also worth exploring in future work; one possible candidate would be a model in which the integration process itself changes with age. 

The developmental noise model reported for experiment 7 offers another way to address the question of what changes with development. This model estimates a developmental trajectory for the proportion of responses that are better explained by random guessing than by the model structure. If such a model would find that model fit is comparable for younger and older children but that the noise parameter through which this fit is achieved decreases with age, we might conclude that cognitive abilities that have to do with task demands are the major locus of change rather than abilities that have to do with integrating information. In the developmental noise model in experiment 7, we found that noise decreased with age but, at the same time, that the resulting model fit was substantially worse for younger children. However, rather than a difference in how information is integrated, we think that a lack of variation in children's responses is the reason for this poor model fit. The strongest evidence for developmental changes in integration would come in a case where younger children showed evidence of above/below-chance judgment in the combined task that was distinct from that predicted by the two above/below-chance component tasks. Such a comparison would require more precision (either via more trials or more participants) than our current experiment affords, however.

Studying how multiple types of pragmatic cues are balanced contributes to a more comprehensive understanding of word learning. In the current study, participants inferred the referent by integrating non-linguistic cues (speakers pointing to a table) with assumptions about speaker informativeness and common ground information, going beyond previous experimental work in measuring how these information sources were combined. The real learning environment is far richer than what we captured in our experimental design, however. For example, in addition to multiple layers of social information, children can rely on semantic and syntactic features of the utterances as cues to meaning [@clark1973s; @gleitman1990structural]. Across development, children learn to recruit these different sources of information and integrate them. RSA models allow for the inclusion of semantic information as part of the utterance [@bergen2016pragmatic] and it will be a fruitful avenue for future research to model the integration of linguistic and pragmatic information across development. To conclude, our work here shows how computational models of language comprehension can be used as powerful tools to explicate and test hypotheses about information integration across development. 

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

\newpage

# Declarations of interest

None.

# Author Contributions

M. Bohn and M.C. Frank conceptualized the study, M. Merrick collected the data, M. Bohn and M.H. Tessler analyzed the data, M. Bohn, M. H. Tessler and M.C. Frank wrote the manuscript, all authors approved the final version of the manuscript.

# Acknowledgments

We thank Jacqueline Quirke and Sabina Zacco for help with the data collection and Bria Long and Gregor Kachel for comments on an earlier version of the paper.

# Funding

M. Bohn received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement no. 749229. M. C. Frank was supported by a Jacobs Foundation Advanced Research Fellowship and the Zhou Fund for Language and Cognition.







