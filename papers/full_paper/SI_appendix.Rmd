---
title             : "Supplementary information:Predicting information integration in pragmatic word learning across development"
shorttitle        : "Supplementary information"

author: 
  - name          : "Manuel Bohn"
    affiliation   : "1,2"
    address       : "Department of Psychology, 450 Serra Mall, Stanford, CA 94305"
    email         : "bohn@stanford.edu"
  - name          : "Michael Henry Tessler"
    affiliation   : "3"
  - name          : "Megan Merrick"
    affiliation   : "1"
  - name          : "Michael C. Frank"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
  - id            : "2"
    institution   : "Leipzig Research Center for Early Child Development, Leipzig University"
  - id            : "3"
    institution   : "Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology"

bibliography      : ["library.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "doc"
output            : papaja::apa6_pdf
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, include = T)
library(papaja)
library(tidyverse)
library(knitr)
library(ggthemes)
library(langcog)
library(rwebppl)
library(coda)
library(matrixStats)
library(ggpubr)
library(lme4)
library(broom)

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}

```


```{r}
# load data files

# adults exp 1 
adult_ex1_data <- read_csv(file="../../stats/data/adult_ex1.csv")
# adults exp 2 novelty
adult_ex2_novelty_data <- read_csv(file="../../stats/data/adult_ex2_novelty.csv")
# adults exp 2 preference
adult_ex2_preference_data <- read_csv(file="../../stats/data/adult_ex2_preference.csv")
# adults ex3
adult_ex3_data <-read_csv(file="../../stats/data/adult_ex3.csv")
# prior strength manipulation experiments in experiment 4
adult_ex4_prior_data <-read_csv(file="../../stats/data/adult_ex4_prior.csv") 
# adults ex4
adult_ex4_data <-read_csv(file="../../stats/data/adult_ex4.csv") 

# children ex1
child_ex1_data <-read_csv(file="../../stats/data/child_ex1.csv")
# children ex2 preference
child_ex2_data <-read_csv(file="../../stats/data/child_ex2.csv") 
# children ex3
child_ex3_data <-read_csv(file="../../stats/data/child_ex3.csv") 
```


# Structure

Here we present details for the cognitive models as well as supplementary analysis and results. Readers who are interested in the model and/or analysis code itself are encouraged to consult the corresponding online repository: (https://github.com/manuelbohn/mcc).  

# Models

Cognitive models were implemented in WebPPL [@dippl] using the r package `rwebppl` [@R-rwebppl].

## Ontology

The situation we model is defined by three sets: referents $r$, utterances $u$, and lexica $\mathcal{L}$. 
Referents are defined by two features: the type of object they are (visually discernible given their shape and color) and their location (one of two tables).
There are two types of objects $t_1$, $t_2$ and two locations $l_1$ and $l_2$.
The utterances available to a speaker are (action, label) pairs, where the action involves turning (pointing) to a particular location (a table) and labels are novel words ($w_1$, $w_2$), which are assumed to refer to the type of the object. 
The action of turning (pointing) reduces the set of referents to those that are on the table targeted by the turn (point); interpreting a label reduces that set further to those which are of the type referred to by that label.
The interpretation of a label requires a lexicon $\mathcal{L}$, which provides the label--type mappings.
There are two lexica: one which maps label 1 $w_1$ to type 1 $t_1$ and label 2 $w_2$ to type 2 $t_2$, and one which has the inverse mapping. 

## Pragmatics model

Our word-learning model is a model of pragmatic reasoning couched in the Rational Speech Act modeling framework. The model describes the following process: A pragmatic listener ($L_1$) jointly infers a referent (what object is being picked out by the utterance) and a lexicon (label--type mappings) by reasoning about a pragmatic speaker ($S_1$) who produces utterances to convey information to a literal listener ($L_0$), who in turn interprets utterances according to their literal meaning. According to Bayes rule, the pragmatic listener's inference is given by:

\begin{equation} 
P_{L_1}(r, \mathcal{L}|u)\propto P_{S_1}(u|r)P( \mathcal{L})P(r)
\end{equation} 

The right-hand side of this equation has three terms: the prior distribution over referents $P(r)$, the prior distribution over lexica $P(\mathcal{L})$, and the likelihood that a speaker would produce an utterance $u$ given a referent $r$ ($P_{S1}(u|r)$).
In the situation we model, the prior on referents $P(r)$ is a categorical distribution over three objects in a scene, which we posit could be non-uniform due to what is in common ground (see below for common ground manipulations). 
Because the labels produced by the speaker are all novel words, the listener has no substantive knowledge about the lexica (label--type mappings) and thus the prior over the two lexica (described above) is uniform.

The pragmatic listener updates their beliefs about both the referent and the lexicon by reasoning about the speaker, assumed to produce utterances to convey the referent to the listener by being a soft-max rational agent (with degree of rationality $\alpha$) with a utility function defined in terms of the informativity of an utterance for a referent:

\begin{equation} 
P_{S_1}(u|r)\propto exp(\alpha Informativity(u;r))
\end{equation} 

As spelled out in @GoodmanStuhlmuller2013, the informativity of an utterance for a referent is the (log) probability that a naive listener $L_0$ would select that referent given that utterance.

\begin{equation} 
Informativity(u; r) = \ln P_{L_0}(r|u)
\end{equation} 

We assume that the speaker believes the literal listener to also be uncertain about the lexicon; that is, the pragmatic listener believes that the speaker (correctly) believes that the listener does not know the lexicon. 
Because the literal listener is assumed to not know the lexicon, the informativity of the utterance averages over the possible lexical, or words meanings, that $L_0$ considers:

\begin{equation} 
P_{L_0}(r|u) = \sum_L P_{L_0}(r, \mathcal{L}|u)
\end{equation} 

where $P_{L_0}(r, \mathcal{L}|u)  \propto \mathcal{L}_{lit} P(r) P(\mathcal{L})$.
$P(r)$ again denotes the prior probability of a referent, $\mathcal{L}_{lit}$ encodes the literal meaning of an utterance given a particular lexicon, returning 1 if the utterance is literally true and 0 if the utterance is literally false. As mentioned above, because $L_1$ does not know the lexicon, the semantics of the words contained in the utterance (i.e., the labels) offer no information about the referent. The non-linguistic aspect of the utterance (the turning or pointing), however, do. As described above, the semantics of turning to one of the tables is roughly equivalent to saying "It's an object on that table". That is, $\mathcal{L}_{lit}$ returns 1 for a referent that is on the table the agent turned to and 0 for referents on the other table.


### Worked Example

TWO CORRECTIONS:
- THE GENERATIVE PROCESS OF THE UTTERANCE: Speaker knows lexicon, but doesn't assume listener does, but must label (we do not model the decision as to whether or not to label). So the only thing informative they do is point.
- QUD is type, not referent. Change L0 model.

In this section, we work through a toy numerical example for how model predictions were generated for the pragmatics model. The prediction will correspond to the parameter free model described above (excluding the noise parameter). The values of the parameters are taken from the preference - same speaker - incongruent condition (see below); that is, the pressures of informativity and preference operate in conflicting directions. Fig. 1 shows a screenshot from the adult experiment, which the model was designed to capture. In this context, there are three potential referents of two types (pink-ish and yellow-ish; for simplicity, we refer refer the object's type by its color) on two tables: $r^{table: 1}_{type:pink}$, $r^{table: 2}_{type:pink}$; one yellow-ish, $r^{table: 2}_{type:yellow}$). For simplicity, we refer to the referents by the shorthand: $r_{p}^1, r_{p}^2, r_y^2$.
In principle, the speaker, the frog in this case, can produce one of four utterances, turning either to the left or right table and saying either label ("dax" or "wug"):  $u_1 = (\text{left}, dax)$, $u_2 = (\text{left}, wug)$, $u_3 = (\text{right}, dax)$, $u_4 = (\text{right}, wug)$.
We work out the example where the speaker produces utterance $u_3$, turning to the right table (two objects) and saying the label "dax" (though since the labels have no *a priori* meanings, the computation would be the same for utterance $u_4$). 


<!-- $u_1$, which is turning to the right table (two objects) and making a request by using a novel label or $u_2$, which involves turning to the left table (single object) and making a request by using a novel label. In the experiment, the speaker chose $u_1$ and we therefore model $L_1$'s interpretation of this utterance. -->

```{r figS1, include = T, fig.align = "center", fig.cap = "Screenshot from adult experiment.", out.width="400px"}
knitr::include_graphics("./figures/SI_setup.png")
```

The listener is learning the mappings between labels and object types (rather than object tokens). That is, the listener either believes the novel word "dax" refers to either "red-ish objects" or "yellow-ish objects". Turning or pointing to a table always has the same meaning. These semantics can be described using two lexica: $\mathcal{L}_1 = \{dax: \text{pink-ish thing}, wug: \text{yellow-ish thing}, point: \text{location of point}\}$, $\mathcal{L}_2 = \{dax: \text{yellow-ish thing}, wug: \text{pink-ish thing}, point: \text{location of point}\}$.

We construct the prior distribution over referents $P(r)$ based on the results of Experiment 2A, in which the speaker displayed a preference for a yellow object (see Model Parameters section below for a detailed description of how this distribution was constructed). 
The prior distribution over referents $P(r)$ (left to right in Fig.1) for this condition was [0.26, 0.26, 0.48].
<!-- [`r round(1-mean(adult_ex2_preference_data%>%filter(condition == "different_speaker")%>%pull(correct)),2)`; `r round(1-mean(adult_ex2_preference_data%>%filter(condition == "different_speaker")%>%pull(correct)),2)`; `r round(mean(adult_ex2_preference_data%>%filter(condition == "different_speaker")%>%pull(correct)),2)`].  -->

We assume that the intentional goal of the speaker is to get the listener to get an option of the correct type. That is, the informativity of an utterance is calculated with respect to conveying the correct object type as opposed to a particular referent (token).
First, we calculate the literal listener's posterior distribution over referents and lexica, and marginalize (average) over objects of the same type and the different lexica to compute the informativity of an utterance for a type.
The literal listener's posterior distribution over referents and lexica has support of size six, owing to the unique combinations of referents and lexica.
$$
\begin{aligned}
& P_{L_0}(r_p^1, \mathcal{L}_1 | {u_3})  \propto \mathcal{L}_1(r_p^1, u_3) P(r_1^r) P(L_1) = 0 \times 0.26 \times 0.5 =0\\
& P_{L_0}(r_p^2, \mathcal{L}_1 | {u_3})  \propto \mathcal{L}_1(r_p^2, u_3) P(r_2^r) P(L_1) = 1 \times 0.26 \times 0.5 =0.13\\
& P_{L_0}(r_y^2, \mathcal{L}_1 | {u_3})  \propto \mathcal{L}_1(r_y^2, u_3) P(r_3^y) P(L_1) = 0 \times 0.48 \times 0.5 =0\\
& P_{L_0}(r_p^1, \mathcal{L}_2 | {u_3})  \propto \mathcal{L}_2(r_p^1, u_3) P(r_1^r) P(L_2) = 0 \times 0.26 \times 0.5=0\\
& P_{L_0}(r_p^2, \mathcal{L}_2 | {u_3})  \propto \mathcal{L}_2(r_p^2, u_3) P(r_2^r) P(L_2) = 0 \times 0.26 \times 0.5=0\\
& P_{L_0}(r_y^2, \mathcal{L}_2 | {u_3})  \propto \mathcal{L}_2(r_y^2, u_3) P(r_3^y) P(L_2) = 1 \times 0.48 \times 0.5=0.24\\
\end{aligned}
$$



because the speaker is pointing to the table with $r_p^2$ and $r_y^2$, and under $\mathcal{L}_1$, "dax" means the pink object and under $\mathcal{L}_2$, "dax" means the yellow object.
After normalization we have:

$$
\begin{aligned}
& P_{L_0}(r_p^1, \mathcal{L}_1 | {u_3}) = 0 \\
& P_{L_0}(r_p^2, \mathcal{L}_1 | {u_3}) = 0.35 \\
& P_{L_0}(r_y^2, \mathcal{L}_1 | {u_3})   = 0 \\
& P_{L_0}(r_p^1, \mathcal{L}_2 | {u_3})   = 0 \\
& P_{L_0}(r_p^2, \mathcal{L}_2 | {u_3})   = 0 \\
& P_{L_0}(r_y^2, \mathcal{L}_2 | {u_3}) = 0.65\\
\end{aligned}
$$
The correspondence of the token distribution (above) with the type distribution is straight-forward:

$$
\begin{aligned}
& P_{L_0}(t_p, \mathcal{L}_1 | {u_3}) = P_{L_0}(r_p^1, \mathcal{L}_1 | {u_3})  + P_{L_0}(r_p^2, \mathcal{L}_1 | {u_3})  = 0+ 0.35 =0.35 \\
& P_{L_0}(t_y, \mathcal{L}_1 | {u_3}) = P_{L_0}(r_y^2, \mathcal{L}_1 | {u_3})  = 0 \\
& P_{L_0}(t_p, \mathcal{L}_2 | {u_3})  = P_{L_0}(r_p^1, \mathcal{L}_2 | {u_3})  + P_{L_0}(r_p^2, \mathcal{L}_2 | {u_3})  = 0+ 0 =0 \\
& P_{L_0}(t_y, \mathcal{L}_2 | {u_3}) = P_{L_0}(r_y^2, \mathcal{L}_2 | {u_3}) = 0.65\\
\end{aligned}
$$

then,

$$
\begin{aligned}
&P_{L_0}(t_p|u_3) =  P_{L_0}(t_p, \mathcal{L}_1|u_3) + P_{L_0}(t_p, \mathcal{L}_2|u_3) = 0.35+0=0.35 \\
&P_{L_0}(t_y|u_3) =  P_{L_0}(t_y, \mathcal{L}_1|u_3) + P_{L_0}(t_y, \mathcal{L}_2|u_3) = 0+0.65=0.65 \\
\end{aligned}
$$

and

$$
\begin{aligned}
&Informativity(u_3; t_p) = \log(0.35) = -1.04 \\
&Informativity(u_3; t_y) = \log(0.65) = -0.43
\end{aligned}
$$

In a similar way, we can compute the informativity of the other utterances. Since the words have no *a priori* meanings, $u_4$ (pointing to the right and saying "wug") will have the same informativity values as $u_3$ (pointing the right and saying "dax"). The informativity vectors for $u_1$ and $u_2$, pointing to the left and saying either "dax" or "wug" are also identical: 

$$
\begin{aligned}
&Informativity(u_1; t_p) = log(P_{L_0}(t_p|u_1)) = \log(1)  = 0\\
&Informativity(u_1; t_y) = log(P_{L_0}(t_y|u_1)) = \log(0) = -\infty
\end{aligned}
$$
We assume that the speaker knows the lexicon, but doesn't believe the listener to know the lexicon. That is, the generative process of the utterance is tantamount to pointing to the table with the referent of the type the speaker wants and incidentally labeling it. The label itself carries no information. 

Recall the two lexica: $\mathcal{L}_1 = \{dax: \text{pink-ish thing}, wug: \text{yellow-ish thing}, point: \text{location of point}\}$, $\mathcal{L}_2 = \{dax: \text{yellow-ish thing}, wug: \text{pink-ish thing}, point: \text{location of point}\}$, and the four utterances:  $u_1 = (\text{left}, dax)$, $u_2 = (\text{left}, wug)$, $u_3 = (\text{right}, dax)$, $u_4 = (\text{right}, wug)$.
We begin with $t_p$. If the speaker's lexicon were $\mathcal{L}_1$:

$$
P_{S_1}(u|t_p, \mathcal{L}_1) \propto \exp(\alpha \cdot Informativity(u;t_p)) = \begin{cases} 
\exp(2.24 \times 0) = 1 & \mbox{for } u_1 \\
\exp(2.24 \times -\infty) = 0& \mbox{for } u_2 \\
\exp(2.24 \times -1.04) = 0.097  & \mbox{for } u_3 \\
\exp(2.24 \times -\infty) = 0& \mbox{for } u_4 \\
\end{cases}
$$
To arrive at the production probabilities, you normalize:

$$
P_{S_1}(u|t_p, \mathcal{L}_1) = \begin{cases} 
0.91  & \mbox{for } u_1 \\
0 & \mbox{for } u_2 \\
0.09  & \mbox{for } u_3 \\
0& \mbox{for } u_4 \\
\end{cases}
$$

The speaker's production probabilities are the same if the speaker's lexicon is $\mathcal{L}_2$, only the utterances use the other label:

$$
P_{S_1}(u|t_p, \mathcal{L}_2) = \begin{cases} 
0  & \mbox{for } u_1 \\
0.91 & \mbox{for } u_2 \\
0  & \mbox{for } u_3 \\
0.09& \mbox{for } u_4 \\
\end{cases}
$$
That is, if the speaker is trying to convey the pinkish object type, the speaker would be 10 times more likely (under a speaker rationality parameter of 2.24) to point to the left table than to the right table. 
If instead the speaker wanted to convey the yellowish object type:
 = -0.43

$$
P_{S_1}(u|t_y, \mathcal{L}_1) = \exp(\alpha \cdot Informativity(u;t_y)) = \begin{cases} 
\exp(2.24 \times -\infty) = 0 & \mbox{for } u_1 \\
\exp(2.24 \times -\infty) = 0& \mbox{for } u_2 \\
\exp(2.24 \times -\infty) = 0  & \mbox{for } u_3 \\
\exp(2.24 \times -0.43) = 0.38& \mbox{for } u_4 \\
\end{cases}
$$

To arrive at the production probabilities, you normalize:

$$
P_{S_1}(u|t_y, \mathcal{L}_1) = \begin{cases} 
0  & \mbox{for } u_1 \\
0 & \mbox{for } u_2 \\
0  & \mbox{for } u_3 \\
1& \mbox{for } u_4 \\
\end{cases}
$$
and

$$
P_{S_1}(u|t_y, \mathcal{L}_2) = \begin{cases} 
0  & \mbox{for } u_1 \\
0 & \mbox{for } u_2 \\
1  & \mbox{for } u_3 \\
0& \mbox{for } u_4 \\
\end{cases}
$$

That is, the speaker would point to the right table and say the utterance consistent with their lexicon. 

Finally, based on equation 1 we can use these values to compute the probability that the listener thinks that the speaker is referring to the yellow object ($r_1$) when they produce $r_1$:

$$
\begin{aligned}
&P_{L_1}(t, \mathcal{L}|u ) = ...
\end{aligned}
$$

Thus, we normalize across all potential states that the utterance could refer to.   




<!-- The utterances available to a speaker are (action, label) pairs, where the action involves turning (or pointing) to a particular location (i.e., a table) and labels are novel words, which are assumed to refer to the type of the object.  -->
<!-- The action of the turning or pointing rules out the referents that are not on the table targeted by the turn (point).  -->
<!-- The listener knows that the labels refer to the kind of object but does not know which kind is referred to be which label, formalized by the prior over lexica $P(L)$.  -->

<!-- In principle, the speaker, the frog in this case, can perform one of two utterances: $u_1$, which is turning to the right table (two objects) and making a request by using a novel label or $u_2$, which involves turning to the left table (single object) and making a request by using a novel label. -->

<!-- For example, in SI Figure 1, two possible labels are "dax" and "wug", each of which could either mean. -->

<!-- Because we studied word learning, we assumed that the listener ($L_1$) had no prior expectations that a particular lexicon  is more likely. In the model, $P(L)$ therefore took the form of a uniform distribution over potential lexica.  -->


### Model parameters  

As noted in the main text, the parameter $\alpha$ (speaker optimality parameter) in equation 2 determines the absolute strength of the likelihood term. It's interpretation is *how* rational $L$ thinks $S$ is in this particular context. For adults, we used the data from Experiment 1 to infer the value of $\alpha$. That is, we inferred which value of $\alpha$ would generate model predictions for the RSA model (assuming equal prior probability for each object) that corresponded to the average proportion of correct responses measured in Experiment 1. This value for $\alpha$ was then used in Experiment 3 and 4.

For children, the speaker optimality parameter changed with age. Instead of inferring a single value across age, we used the data from Experiment 5 to find the slope and intercept for $\alpha$ that best described the developmental trajectory in the data. As for adults, this was done via the RSA model with equal prior probability for each object. In Experiment 7, the speaker optimality parameter for a given child of a given age was computed by taking the overall intercept and adding the slope times the child's age (with age anchored at 0).

The prior distribution over objects, $P(r)$, varied with the common ground manipulation, the identity of the speaker and the alignment of utterance and common ground information. Numerically, it depended on the measurement obtained in Experiment 2A and B for adults and Experiment 6 for children. 

For adults, this worked in the following way: For example, in Experiment 2, for the preference/same speaker condition, when the speaker indicated that they liked object A and disliked object B, the average proportion with which participants chose object A was `r adult_ex2_preference_data %>% filter(condition == "same_speaker") %>% summarise(mean = mean(correct)) %>% pull(mean)` and for object B it was `r 1-adult_ex2_preference_data %>% filter(condition == "same_speaker") %>% summarise(mean = mean(correct)) %>% pull(mean)` respectively. In Experiment 3, this measurement determined the prior distribution over objects in cases whenever the the same manipulation was used (preference/same speaker). When utterance and common ground information were aligned (i.e. object A was the more informative object), the distribution of objects was (A,B,B). The corresponding prior distribution was therefore (`r adult_ex2_preference_data %>% filter(condition == "same_speaker") %>% summarise(mean = mean(correct)) %>% pull(mean)`, `r 1-adult_ex2_preference_data %>% filter(condition == "same_speaker") %>% summarise(mean = mean(correct)) %>% pull(mean)`, `r 1-adult_ex2_preference_data %>% filter(condition == "same_speaker") %>% summarise(mean = mean(correct)) %>% pull(mean)`). When information sources were dis-aligned (i.e. object B was the more informative one), the object distribution was (B,A,A) and the prior distribution was (`r 1-adult_ex2_preference_data %>% filter(condition == "same_speaker") %>% summarise(mean = mean(correct)) %>% pull(mean)`, `r adult_ex2_preference_data %>% filter(condition == "same_speaker") %>% summarise(mean = mean(correct)) %>% pull(mean)`, `r adult_ex2_preference_data %>% filter(condition == "same_speaker") %>% summarise(mean = mean(correct)) %>% pull(mean)`). Note that Experiment 3 involved three objects while Experiment 2 only involved two. We nevertheless used the exact proportions measured in Experiment 2 for each object to inform the prior. This approach spread out the absolute probability mass but conserved the relative relation between objects.

For children, we used the data from Experiment 6 to model the slope and intercept that best described the developmental trajectory in the data for each of the two conditions. As for the speaker optimality parameter, this allowed us to generate prior distributions that were sensitive to the child's age. In Experiment 7, the prior probability for an object was computed by taking the intercept for the respective condition (same or different speaker), adding the slope times the child's age and then using a logistic transformation to convert the outcome into proportions. The overall distribution then depended on the alignment of information sources in the same way as it did for adults. 

## Prior only model

The prior only model ignored the information about the intended referent that was expressed by the utterance and instead only focused only on common ground manipulation. It is defined as: 

\begin{equation} 
P_{L}(r|u)\propto P(r)
\end{equation} 

That is, the probability of the referent given the utterance is determined by the prior probability of the referent for a particular speaker. The prior distributions were set in the same way as for the pragmatics model.

## Flat prior model

This model was identical in structure to the pragmatics model with the exception that the prior distribution did not correspond to the measurements from Experiment 2 and did not vary with speaker identity. That is, regardless of common ground manipulation and speaker identity the prior distribution was always uniform (e.g. 0.33,0.33,0.33). This was the case for children as well as  adults. The speaker optimality parameter was set in the same way as in the pragmatics model. 

# Prior strength manipualtions Experiment 4

```{r ex4 prior pre-tests data adults}
# load data from prior strength manipulation experiments to set priors
adult_ex4_prior_data <-read_csv(file="../../stats/data/adult_ex4_prior.csv") 
```

```{r table_prior,results = "asis"}
ex4_prior_models <- adult_ex4_prior_data %>%
  group_by(common_ground_manipulation,prior_manipulation)%>%
  do(models = glmer(correct ~ speaker + (1|id) + (1|agent),data = .,family=binomial,  control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5))))


ex4_prior_models_table <- tidy(ex4_prior_models, models)%>%
  filter(group == "fixed")%>%
  select(-statistic, -group)%>%
  mutate(p.value = ifelse(p.value<.001,"< .001",as.character(paste("=",substr(round(p.value,3),2,5),sep=" "))),
         std.error = round(std.error,2),
         estimate = round(estimate,2),
         term = case_when(term == "(Intercept)" ~ "Intercept",
                          term == "speakersame_speaker" ~ "condition (same speaker)"))%>%
  rename(Manipulation = common_ground_manipulation,
         Strength = prior_manipulation,
         Term = term,
         Estimate = estimate,
         SE = std.error,
         p = p.value)

apa_table(
  ex4_prior_models_table
  , caption = "Model output for prior strength experiments "
  , note = "Model structure in all cases: correct ~ condition + (1|id) + (1|agent)"
  , escape = T
)
```

Below we describe the different ways in which prior strength was manipulated in Experiment 4. The corresponding experiments can be found in the online repository. The test event was always the same: The animal disappeared and then either the same or a different animal returned and requested an object using an unknown word.

For preference, both tables initially contained an object. In preference/strong the animal turned to one side and stated that they liked (“Oh wow, I really like that one”) or disliked (“Oh bleh, I really don’t like that one”) the object. Then they turned the other side and expressed the respective other attitude. In preference/medium the animal only turned to one side and expressed liking in a more subtle way (saying only: “Oh, wow”). 

For novelty, one table was empty while there was an object on the other. In novelty/strong the animal turned to one of the sides and commented either on the presence (“Aha, look at that”) or the absence of an object (“Hm, nothing there”). Then the animal turned to the other side and commented in a complementary way. Next, the animal disappeared. The same animal re-appeared and the sequence above was repeated. When the animal disappeared for the second time, a second object appeared on the empty table while the animal was away. In novelty/medium, the animal commented on the presence/absence of objects in the same way but did so only once. In novelty/weak, the animal only turned to the present object and commented on it. 

In all cases, the order of utterances and/or the side to which the speaker turned first were counterbalanced. Figure \ref{fig:priorplot} shows the results for the same speaker and different speaker conditions for each manipulation.

```{r priorplot, fig.cap = "Results from prior strength manipulation Experiments. Transparent dots show data from individual participants, diamonds represent condition means, error bars are 95\\% CIs. Dashed line indicates performance expected by chance.", fig.height = 3}

p_adult_ex4_priors <- adult_ex4_prior_data%>%
  mutate(prior_manipulation = relevel(as.factor(prior_manipulation), ref = "strong"))%>%
  group_by(common_ground_manipulation,prior_manipulation,speaker, id) %>%
  summarise(correct = mean(correct))
  

p_adult_ex4_priors_ci <- p_adult_ex4_priors %>%
  multi_boot_standard(col = "correct")

ggplot() +
  geom_jitter(data = p_adult_ex4_priors, aes(x = speaker, y = correct, col = speaker), alpha = 0.3, width = .3,height = .02)+
  geom_pointrange(data = p_adult_ex4_priors_ci, aes(x = speaker, y = mean, col = speaker,ymin = ci_lower, ymax = ci_upper),size = .8, pch = 5, fatten = 2.5)+
  geom_hline(yintercept = 0.5, lty=2)+
  labs(x="",y="")+
  facet_grid(common_ground_manipulation ~ prior_manipulation)+
  theme_few() + 
  ylim(-0.05,1.05)+
  guides(alpha = F,size =F)+
  scale_color_solarized()+
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), legend.position = "right")
```

Table \ref{tab:table_prior} shows the results of a generalized linear mixed model (GLMM) fit to the data from each manipulation. The results show that the parameter estimates for condition (i.e. difference between same speaker and different speaker condition) decreases in line with the hypothesized effect of the prior manipulation. 

# Model comparison

Analysis code for model comparison can be found in the online repository.

## Experiment 3

Here we report details on the model comparisons. Model fit was assessed based on marginal log-likelihoods of the data under each model. Bayes Factors were computed by first subtracting log-likelihoods and then exponentiating the result. Table \ref{tab:ex3_comp} shows Bayes Factors for model comparisons in Experiment 3. We did not pre-register the inclusion of the noise parameter for Experiment 3, but did so for all subsequent experiments for which we did model comparisons (4 and 7). The first row in Table \ref{tab:ex3_comp} compares the pragmatics model with noise parameter to the model without the noise parameter. This comparison shows that including the noise parameter greatly improves model fit. Figure \ref{fig:modelpredex3} compares model predictions from the models including noise parameters to the data from Experiment 3. 

```{r modelpredex3, include = T, fig.cap = "Correlation plot for model predictions and data from Experiment 3. All models depicted here included a noise parameter. Coefficients and p-values are based on Pearson correlation statistics. Dots represent condition modes. Error bars represent 95\\% HDIs.", fig.height=2.5}

adult_ex3_data_summary <- adult_ex3_data %>%
  mutate(model = "data") %>%
  group_by(model,common_ground_manipulation,speaker,alignment)%>%
  summarize(k = sum(correct_inf), n = n())%>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         ci_lower  = qbeta(.025, a, b),
         ci_upper = qbeta(.975, a, b),
         mean = (a-1)/(a+b-2))%>%
  select(-a,-b,-n,-k)

# summarize model predictions

ex3_flat_prior_model_pred_noise <- readRDS("../../stats/saves/ex3_flat_prior_model_noise.rds") %>%
  filter(!(Parameter %in% c("noise")))  %>%
  separate(Parameter, into = c("common_ground_manipulation","speaker", "alignment"), sep="/")%>%
  mutate(model="flat_prior_noise")%>%
  group_by(model,common_ground_manipulation,speaker,alignment)%>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))

ex3_prior_only_model_pred_noise <- readRDS("../../stats/saves/ex3_prior_only_model_noise.rds") %>%
  filter(!(Parameter %in% c("noise")))  %>%
  separate(Parameter, into = c("common_ground_manipulation","speaker", "alignment"), sep="/")%>%
  mutate(model="prior_only_noise")%>%
  group_by(model,common_ground_manipulation,speaker,alignment)%>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))

ex3_pragm_model_pred_noise <- readRDS("../../stats/saves/ex3_pragm_model_noise.rds") %>%
  filter(!(Parameter %in% c("noise")))  %>%
  separate(Parameter, into = c("common_ground_manipulation","speaker", "alignment"), sep="/")%>%
  mutate(model="pragmatic_noise")%>%
  group_by(model,common_ground_manipulation,speaker,alignment)%>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))


ex_3_model_pred <- bind_rows(
  ex3_flat_prior_model_pred_noise,
  ex3_prior_only_model_pred_noise,
  ex3_pragm_model_pred_noise
)
 
# combine model predictions and data

ex_3_pred <- bind_rows(
  ex_3_model_pred,
  adult_ex3_data_summary
)

ex_3_cor_plot <- ex_3_pred %>%
  ungroup()%>%
  filter(model != "data") %>%
  left_join(., ex_3_pred %>%
    ungroup()%>%
    filter(model == "data") %>%
    rename(data_mean = mean, data_ci_lower = ci_lower, data_ci_upper = ci_upper) %>%
    select(-model)
  )%>%
  mutate(model =  case_when(model == "pragmatic_noise" ~ "Pragmatics",
                            model == "prior_only_noise" ~ "Prior only",
                            model == "flat_prior_noise" ~ "Flat prior"
                            ))


ggplot(data = ex_3_cor_plot,aes(x = mean, y = data_mean, col = "1")) +
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 0.7, size = 0.5)+
  geom_point(size = 2)+
  geom_errorbar(aes(ymin = data_ci_lower, ymax = data_ci_upper),width = 0,size = .7)+
  geom_errorbarh(aes(xmin = ci_lower, xmax = ci_upper), height = 0,size = .7)+
  coord_fixed()+
  xlim(0,1)+ylim(0,1)+
  xlab("Model")+
  ylab("Data")+
  facet_grid ( ~ model) +
  guides(col = F)+
  stat_cor(method = "pearson", label.x = 0.01, label.y = 0.99, aes(x = mean, y = data_mean), inherit.aes = F, size = 3)+
  theme_few(base_size = )+
  scale_color_manual(values = c("#4477AA"))+
  theme(legend.position = "right")

```

```{r ex3_comp,results = "asis"}
log_like_pragmatic_ex3 <- readRDS("../../stats/saves/ex3_pragm_model_loglike.rds")

ex3_pragm_model_noise_loglike <- readRDS("../../stats/saves/ex3_pragm_model_noise_loglike.rds")%>%
  mutate(model = "pragmatic_noise")%>%
  group_by(model)%>%
  summarize(logP = logSumExp(value))

ex3_prior_only_model_noise_loglike <- readRDS("../../stats/saves/ex3_prior_only_model_noise_loglike.rds")%>%
  mutate(model = "prior_only_noise")%>%
  group_by(model)%>%
  summarize(logP = logSumExp(value))

ex3_flat_prior_model_noise_loglike <- readRDS("../../stats/saves/ex3_flat_prior_model_noise_loglike.rds")%>%
  mutate(model = "flat_prior_noise")%>%
  group_by(model)%>%
  summarize(logP = logSumExp(value))

ex3_model_comparison <- bind_rows(
  log_like_pragmatic_ex3,
  ex3_pragm_model_noise_loglike,
  ex3_flat_prior_model_noise_loglike,
  ex3_prior_only_model_noise_loglike
)%>%
  mutate(model = reorder(model,logP))

ex3_bf <- bind_rows(
  log_like_pragmatic_ex3,
  ex3_pragm_model_noise_loglike,
  ex3_flat_prior_model_noise_loglike,
  ex3_prior_only_model_noise_loglike
) %>% spread(model, logP) %>%
  mutate("pragmatic_noise > pragmatic" = exp(pragmatic_noise - pragmatic),
         "pragmatic_noise > prior_only_noise" = exp(pragmatic_noise - prior_only_noise),
         "pragmatic_noise > flat_prior_noise" = exp(pragmatic_noise - flat_prior_noise),
         "prior_only_noise > flat_prior_noise" = exp(prior_only_noise - flat_prior_noise)) %>%
  select(-pragmatic,-pragmatic_noise, -flat_prior_noise, -prior_only_noise)%>%
  gather(Comparison, BF)%>%
  mutate(BF = formatC(BF,2))

apa_table(
  ex3_bf
  , caption = "Bayes Factors for model comparisons in Experiment 3"
  , escape = T
)

```

```{r ex3noise, include = T, fig.cap = "Posterior distribution of noise parameter for each model in Experiment 4", fig.height = 2}
ex3_noise_parameters <- bind_rows(
  readRDS("../../stats/saves/ex3_pragm_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  mutate(Model = "Pragmatics"),
  readRDS("../../stats/saves/ex3_prior_only_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  mutate(Model = "Prior only"),
  readRDS("../../stats/saves/ex3_flat_prior_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  mutate(Model = "Flat prior")
  )

ggplot(ex3_noise_parameters, aes(x = value, col = Model, fill = Model))+
  geom_density(alpha = .5)+
  theme_few()+
  xlab("Noise parameter")+
  xlim(0,1)+
  scale_color_manual(breaks=c("Pragmatics","Prior only","Flat prior"),
                    values= c("#b58900","#6c71c4","#859900"))+
  scale_fill_manual(breaks=c("Pragmatics","Prior only","Flat prior"),
                    values= c("#b58900","#6c71c4","#859900"))
```

Figure \ref{fig:ex3noise} shows the posterior distribution of the noise parameter for each model. The noise parameter was fit to the data and indicates the proportion of responses that are estimated to be due to random guessing rather than in line with model predictions. Consequently, a model that makes predictions that are closer to the data is likely to have a lower noise parameter. 

## Experiment 4

```{r modelpredex4, include = T, fig.cap = "Correlation plot for model predictions and data from Experiment 4. All models included a noise parameter. Coefficients and p-values are based on Pearson correlation statistics. Dots represent condition modes. Error bars represent 95\\% HDIs.", fig.height=3}

adult_ex4_data_summary <- adult_ex4_data %>%
  mutate(prior_manipulation = relevel(as.factor(prior_manipulation), ref = "strong"))%>%
  mutate(model = "data") %>%
  group_by(model,common_ground_manipulation,prior_manipulation,speaker,alignment)%>%
  summarize(k = sum(correct_inf), n = n())%>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         ci_lower  = qbeta(.025, a, b),
         ci_upper = qbeta(.975, a, b),
         mean = (a-1)/(a+b-2))%>%
  select(-a,-b,-n,-k)

# summarize model predictions

ex4_flat_prior_model_pred_noise <- readRDS("../../stats/saves/ex4_flat_prior_model_noise.rds") %>%
  filter(!(Parameter %in% c("noise")))  %>%
  separate(Parameter, into = c("common_ground_manipulation","prior_manipulation","speaker", "alignment"), sep="/")%>%
  mutate(model="flat_prior_noise")%>%
  group_by(model,common_ground_manipulation,prior_manipulation,speaker,alignment)%>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))

ex4_prior_only_model_pred_noise <- readRDS("../../stats/saves/ex4_prior_only_model_noise.rds") %>%
  filter(!(Parameter %in% c("noise")))  %>%
  separate(Parameter, into = c("common_ground_manipulation","prior_manipulation","speaker", "alignment"), sep="/")%>%
  mutate(model="prior_only_noise")%>%
  group_by(model,common_ground_manipulation,prior_manipulation,speaker,alignment)%>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))

ex4_pragm_model_pred_noise <- readRDS("../../stats/saves/ex4_pragm_model_noise.rds") %>%
  filter(!(Parameter %in% c("noise")))  %>%
  separate(Parameter, into = c("common_ground_manipulation","prior_manipulation","speaker", "alignment"), sep="/")%>%
  mutate(model="pragmatic_noise")%>%
  group_by(model,common_ground_manipulation,prior_manipulation,speaker,alignment)%>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))


ex_4_model_pred <- bind_rows(
  ex4_pragm_model_pred_noise,
  ex4_prior_only_model_pred_noise,
  ex4_flat_prior_model_pred_noise
)%>%
  ungroup()%>%
  mutate(prior_manipulation = relevel(as.factor(prior_manipulation), ref = "strong"))
 
# combine model predictions and data

ex_4_pred <- bind_rows(
  ex_4_model_pred,
  adult_ex4_data_summary
)

ex_4_cor_plot <- ex_4_pred %>%
  ungroup()%>%
  filter(model != "data") %>%
  left_join(., ex_4_pred %>%
    ungroup()%>%
    filter(model == "data") %>%
    rename(data_mean = mean, data_ci_lower = ci_lower, data_ci_upper = ci_upper) %>%
    select(-model)
  )%>%
  mutate(model =  case_when(model == "pragmatic_noise" ~ "Pragmatics",
                            model == "prior_only_noise" ~ "Prior only",
                            model == "flat_prior_noise" ~ "Flat prior"
                            ))


ggplot(data = ex_4_cor_plot,aes(x = mean, y = data_mean, col = prior_manipulation)) +
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 0.7, size = 0.5)+
  geom_point(size = 2)+
  geom_errorbar(aes(ymin = data_ci_lower, ymax = data_ci_upper),width = 0,size = .7)+
  geom_errorbarh(aes(xmin = ci_lower, xmax = ci_upper), height = 0,size = .7)+
  coord_fixed()+
  xlim(0,1)+ylim(0,1)+
  xlab("Model")+
  ylab("Data")+
  facet_grid ( ~ model) +
  stat_cor(method = "pearson", label.x = 0.01, label.y = 0.99, aes(x = mean, y = data_mean), inherit.aes = F, size = 3)+
  theme_few(base_size = ) + 
  scale_colour_ptol(name = "Prior strength")+
  theme(legend.position = "bottom")

```

```{r ex4_comp, results="asis"}
ex4_pragm_model_noise_loglike <- readRDS("../../stats/saves/ex4_pragm_model_noise_loglike.rds")%>%
  mutate(model = "pragmatic_noise")%>%
  group_by(model)%>%
  summarize(logP = logSumExp(value))

ex4_prior_only_model_noise_loglike <- readRDS("../../stats/saves/ex4_prior_only_model_noise_loglike.rds")%>%
  mutate(model = "prior_only_noise")%>%
  group_by(model)%>%
  summarize(logP = logSumExp(value))

ex4_flat_prior_model_noise_loglike <- readRDS("../../stats/saves/ex4_flat_prior_model_noise_loglike.rds")%>%
  mutate(model = "flat_prior_noise")%>%
  group_by(model)%>%
  summarize(logP = logSumExp(value))

ex4_bf <- bind_rows(
  ex4_pragm_model_noise_loglike,
  ex4_flat_prior_model_noise_loglike,
  ex4_prior_only_model_noise_loglike
) %>% spread(model, logP) %>%
  mutate("Pragmatics > Prior only" = exp(pragmatic_noise - prior_only_noise),
         "Pragmatics > Flat prior" = exp(pragmatic_noise - flat_prior_noise),
         "Flat prior > Prior only" = exp(flat_prior_noise - prior_only_noise)) %>%
  select(-pragmatic_noise, -flat_prior_noise, -prior_only_noise)%>%
  gather(Comparison, BF)%>%
  mutate(BF = formatC(BF,2))

apa_table(
  ex4_bf
  , caption = "Model comparisons in Experiment 4"
  , note = "BF = Bayes Factor; All models include a noise parameter."
  , escape = T
)


```

```{r ex4noise, include = T, fig.cap = "Posterior distribution of noise parameter for each model in Experiment 4", fig.height = 2}
ex4_noise_parameters <- bind_rows(
  readRDS("../../stats/saves/ex4_pragm_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  mutate(Model = "Pragmatics"),
  readRDS("../../stats/saves/ex4_prior_only_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  mutate(Model = "Prior only"),
  readRDS("../../stats/saves/ex4_flat_prior_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  mutate(Model = "Flat prior")
  )

ggplot(ex4_noise_parameters, aes(x = value, col = Model, fill = Model))+
  geom_density(alpha = .5)+
  theme_few()+
  xlab("Noise parameter")+
  xlim(0,1)+
  scale_color_manual(breaks=c("Pragmatics","Prior only","Flat prior"),
                    values= c("#b58900","#6c71c4","#859900"))+
  scale_fill_manual(breaks=c("Pragmatics","Prior only","Flat prior"),
                    values= c("#b58900","#6c71c4","#859900"))
```

Figure \ref{fig:modelpredex4} compares model predictions to the data from Experiment 4. Table \ref{tab:ex4_comp} shows Bayes Factors for model comparisons in Experiment 3. As pre-registered, all models included a noise parameter. Figure \ref{fig:ex4noise} shows the posterior distribution of the noise parameter for each model in Experiment 4.

## Experiment 7

For children, we compared models using different types of noise parameters. We preregistered the model comparison for models including a single noise parameter. We added the additional model comparisons because the noise parameter was comparably high. The additional model comparisons allow us to see if the pragmatics model provides a better fit when more emphasis is put on the model structure itself. The results show that this was the case.

Parameter free models did not include a noise parameter. Noise models included a single noise parameter across age. Developmental noise models included a noise parameter that changed with age. That is, instead of a single value, we inferred an intercept and a slope for the noise parameter. Noise was therefore a function of the child's age. Table \ref{tab:child_prag_bf} shows model comparisons for the pragmatics models using different noise parameters. This shows that including a noise parameter improves model fit but that the type of noise parameter does not make much of a difference.  

```{r child_prag_bf, results = "asis"}
child_ex3_model_comparison <- readRDS("../../stats/saves/child_ex3_model_comparison.rds")

child_ex3_pragmatic_models_bf <- child_ex3_model_comparison%>%
  filter(model == "pragmatic")%>%
  spread(parameter, logP) %>%
  mutate("dev. noise > noise" = exp(`developmental noise` - noise),
         "noise > parameter free" = exp(noise - `parameter free`),
         "dev. noise > parameter free" = exp(`developmental noise` - `parameter free`)) %>%
  select(-model,-`parameter free`,-`developmental noise`, -noise)%>%
  gather(Comparison, BF)%>%
  mutate(BF = formatC(BF,2))

apa_table(
  child_ex3_pragmatic_models_bf
  , caption = "Model comparisons for pragmatics models in Experiment 7"
  , note = "BF = Bayes Factor"
  , escape = T
)

```

Table \ref{tab:child_bf} shows results for model comparison for the different types of noise parameters. In all cases, the pragmatics model provides a substantially better fit to the data compared to the alternative models.

```{r child_bf, results = "asis"}
child_ex3_bf <- child_ex3_model_comparison%>%
  rename(Parameter = parameter)%>%
  group_by(Parameter)%>%
  spread(model, logP) %>%
  mutate("Pragmatics > Flat P." = formatC(exp(pragmatic - flat_prior),2),
         "Pragmatics > P. only" = formatC(exp(pragmatic - prior_only),2),
         "Flat P. > P. only" = formatC(exp(flat_prior - prior_only),2)) %>%
  select(-pragmatic,-flat_prior, -prior_only)

apa_table(
  child_ex3_bf
  , caption = "Model comparisons in Experiment 7"
  , note = "BF = Bayes Factor"
  , escape = T
)

```

```{r}
child_noise_parameters <- bind_rows(
  readRDS("../../stats/saves/child_ex3_pragm_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  mutate(Model = "Pragmatics"),
  readRDS("../../stats/saves/child_ex3_prior_only_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  mutate(Model = "Prior only"),
  readRDS("../../stats/saves/child_ex3_flat_prior_model_noise.rds") %>%
  filter(Parameter %in% c("noise"))  %>%
  mutate(Model = "Flat prior")
  )

child_noise_plot <- ggplot(child_noise_parameters, aes(x = value, col = Model, fill = Model))+
  geom_density(alpha = .5)+
  theme_few()+
  xlab("Noise parameter")+
  xlim(0,1)+
  scale_color_manual(breaks=c("Pragmatics","Prior only","Flat prior"),
                    values= c("#b58900","#6c71c4","#859900"))+
  scale_fill_manual(breaks=c("Pragmatics","Prior only","Flat prior"),
                    values= c("#b58900","#6c71c4","#859900"))
```

```{r, include = T}

child_ex3_pragm_model_developmental_noise_param <- readRDS("../../stats/saves/child_ex3_pragm_model_developmental_noise.rds") %>%
  filter(Parameter %in% c("noise_int","noise_slope"))  %>%
  group_by(Parameter)%>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))

child_ex3_prior_only_model_developmental_noise_param <- readRDS("../../stats/saves/child_ex3_prior_only_model_developmental_noise.rds") %>%
  filter(Parameter %in% c("noise_int","noise_slope"))  %>%
  group_by(Parameter)%>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))


child_ex3_flat_prior_model_developmental_noise_param <- readRDS("../../stats/saves/child_ex3_flat_prior_model_developmental_noise.rds") %>%
  filter(Parameter %in% c("noise_int","noise_slope"))  %>%
  group_by(Parameter)%>%
  summarise(mean = estimate_mode(value), 
            ci_lower = hdi_lower(value),
            ci_upper = hdi_upper(value))

developmental_noise_parameter <- data_frame(
  age = rep(seq(0,2, by = 0.05),3),
  Model = c(rep("Pragmatics",41),
            rep("Flat prior",41),
            rep("Prior only",41)),
  int = c(rep(child_ex3_pragm_model_developmental_noise_param%>%
                filter(Parameter == "noise_int")%>%
                pull(mean),41),
          rep(child_ex3_flat_prior_model_developmental_noise_param%>%
                filter(Parameter == "noise_int")%>%
                pull(mean),41),
          rep(child_ex3_prior_only_model_developmental_noise_param%>%
                filter(Parameter == "noise_int")%>%
                pull(mean),41)
          ),
  slope = c(rep(child_ex3_pragm_model_developmental_noise_param%>%
                filter(Parameter == "noise_slope")%>%
                pull(mean),41),
          rep(child_ex3_flat_prior_model_developmental_noise_param%>%
                filter(Parameter == "noise_slope")%>%
                pull(mean),41),
          rep(child_ex3_prior_only_model_developmental_noise_param%>%
                filter(Parameter == "noise_slope")%>%
                pull(mean),41))
) %>%
  mutate(y = plogis(int + slope *age),
         age = age +3)

child_dev_noise_plot <- ggplot(developmental_noise_parameter, aes(x = age, y= y, col = Model))+
  geom_line(size = 1)+
  ylab("Noise parameter")+
  xlab("Age")+
  ylim(0,1)+
  theme_few()+
  scale_color_manual(breaks=c("Pragmatics","Prior only","Flat prior"),
                    values= c("#b58900","#6c71c4","#859900"))
```

Figure \ref{fig:childnoise} shows the different types of noise parameters for the each model. Figure \ref{fig:childnoise}A shows that the pragmatics model has the lowest estimated level of noise of all the models considered. Figure \ref{fig:childnoise}B shows that the the pragmatics model has the lowest level of estimated noise across the entire age range. It also shows that noise decreases with age for the pragmatics model, suggesting that older children behaved more in line with model predictions compared to younger children.

```{r childnoise, include = T, fig.cap = "Posterior distribution of noise parameter for each model in Experiment 7. A: single noise parameter across age, B: Developmental noise parameter.", fig.height = 2.5}
ggarrange(child_noise_plot,child_dev_noise_plot, nrow = 1, ncol = 2, common.legend = T, legend = "right", labels = c("A","B"))
```

Finally, Figure \ref{fig:childcor} shows correlations between model predictions and the data, binned by year. Across noise parameters, model predictions and data are closest aligned (i.e. closest to the dotted line) for the pragmatics model, thereby corroborating the conclusions drawn based on the model comparison and the evaluation of the noise parameters. 

```{r childcor,  include = T, fig.cap = "Correlation plot for model predictions and data for all models considered in Experiment 7. Dots represent condition modes. Error bars represent 95\\% HDIs.", fig.height = 7}

# child_cor <- bind_rows(
#   readRDS("../../stats/saves/child_ex3_pragm_model_noise.rds") %>%
#   filter(!(Parameter %in% c("noise"))) %>%
#   separate(Parameter, into = c("age", "speaker", "alignment"), sep="/")%>%
#   mutate(age = ifelse(age<4,"3-year-olds","4-year-olds"))%>%
#   mutate(model = "Pragmatic",
#          noise = "Noise")%>%
#   group_by(model, noise,age,alignment,speaker) %>%
#   summarise(mean = estimate_mode(value), 
#             ci_lower = hdi_lower(value), 
#             ci_upper =hdi_upper(value)),
# 
#   readRDS("../../stats/saves/child_ex3_flat_prior_model_noise.rds") %>%
#   filter(!(Parameter %in% c("noise"))) %>%
#   separate(Parameter, into = c("age", "speaker", "alignment"), sep="/")%>%
#   mutate(age = ifelse(age<4,"3-year-olds","4-year-olds"))%>%
#   mutate(model = "Flat prior",
#          noise = "Noise")%>%
#   group_by(model, noise,age,alignment,speaker) %>%
#   summarise(mean = estimate_mode(value), 
#             ci_lower = hdi_lower(value), 
#             ci_upper =hdi_upper(value)),
#   
#   readRDS("../../stats/saves/child_ex3_prior_only_model_noise.rds") %>%
#   filter(!(Parameter %in% c("noise"))) %>%
#   separate(Parameter, into = c("age", "speaker", "alignment"), sep="/")%>%
#   mutate(age = ifelse(age<4,"3-year-olds","4-year-olds"))%>%
#   mutate(model = "Prior only",
#          noise = "Noise")%>%
#   group_by(model, noise,age,alignment,speaker) %>%
#   summarise(mean = estimate_mode(value), 
#             ci_lower = hdi_lower(value), 
#             ci_upper =hdi_upper(value)),
#   
#     readRDS("../../stats/saves/child_ex3_pragm_model_developmental_noise.rds") %>%
#   filter(!(Parameter %in% c("noise"))) %>%
#   separate(Parameter, into = c("age", "speaker", "alignment"), sep="/")%>%
#   mutate(age = ifelse(age<4,"3-year-olds","4-year-olds"))%>%
#   mutate(model = "Pragmatic",
#          noise = "Dev. noise")%>%
#   group_by(model, noise,age,alignment,speaker) %>%
#   summarise(mean = estimate_mode(value), 
#             ci_lower = hdi_lower(value), 
#             ci_upper =hdi_upper(value)),
# 
#   readRDS("../../stats/saves/child_ex3_flat_prior_model_developmental_noise.rds") %>%
#   filter(!(Parameter %in% c("noise"))) %>%
#   separate(Parameter, into = c("age", "speaker", "alignment"), sep="/")%>%
#   mutate(age = ifelse(age<4,"3-year-olds","4-year-olds"))%>%
#   mutate(model = "Flat prior",
#          noise = "Dev. noise")%>%
#   group_by(model, noise,age,alignment,speaker) %>%
#   summarise(mean = estimate_mode(value), 
#             ci_lower = hdi_lower(value), 
#             ci_upper =hdi_upper(value)),
#   
#   readRDS("../../stats/saves/child_ex3_prior_only_model_developmental_noise.rds") %>%
#   filter(!(Parameter %in% c("noise"))) %>%
#   separate(Parameter, into = c("age", "speaker", "alignment"), sep="/")%>%
#   mutate(age = ifelse(age<4,"3-year-olds","4-year-olds"))%>%
#   mutate(model = "Prior only",
#          noise = "Dev. noise")%>%
#   group_by(model, noise,age,alignment,speaker) %>%
#   summarise(mean = estimate_mode(value), 
#             ci_lower = hdi_lower(value), 
#             ci_upper =hdi_upper(value)),
#   
#   
#   readRDS("../../stats/saves/child_ex3_pragm_model_param_free.rds") %>%
#   separate(`0`, into = c("age", "speaker", "alignment","correct"), sep="/")%>%
#   mutate(value = `1`,
#          age = as.numeric(age))%>%
#   select(-`1`)%>%
#   mutate(age = ifelse(age<4,"3-year-olds","4-year-olds"))%>%
#   mutate(model = "Pragmatic",
#          noise = "Param. free")%>%
#   group_by(model,noise, age,alignment,speaker) %>%
#   summarise(mean = estimate_mode(value), 
#             ci_lower = hdi_lower(value), 
#             ci_upper =hdi_upper(value)),
#   
#   readRDS("../../stats/saves/child_ex3_prior_only_model_param_free.rds") %>%
#   separate(`0`, into = c("age", "speaker", "alignment","correct"), sep="/")%>%
#   mutate(value = `1`,
#          age = as.numeric(age))%>%
#   select(-`1`)%>%
#   mutate(age = ifelse(age<4,"3-year-olds","4-year-olds"))%>%
#   mutate(model = "Prior only",
#          noise = "Param. free")%>%
#   group_by(model,noise, age,alignment,speaker) %>%
#   summarise(mean = estimate_mode(value), 
#             ci_lower = hdi_lower(value), 
#             ci_upper =hdi_upper(value)),
#   
#   readRDS("../../stats/saves/child_ex3_flat_prior_model_param_free.rds") %>%
#   separate(`0`, into = c("age", "speaker", "alignment","correct"), sep="/")%>%
#   mutate(value = `1`,
#          age = as.numeric(age))%>%
#   select(-`1`)%>%
#   mutate(age = ifelse(age<4,"3-year-olds","4-year-olds"))%>%
#   mutate(model = "Flat prior",
#          noise = "Param. free")%>%
#   group_by(model,noise, age,alignment,speaker) %>%
#   summarise(mean = estimate_mode(value), 
#             ci_lower = hdi_lower(value), 
#             ci_upper =hdi_upper(value))
# 
# )

p_child_ex3_data <- child_ex3_data %>%
  mutate(age = as.character(age_bin), 
         model = "Data")%>%
  group_by(model,age, alignment, speaker) %>%
  summarize(k = sum(correct_inf), n = n())%>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         mean = (a-1)/(a+b-2),
         ci_lower  = qbeta(.025, a, b),
         ci_upper = qbeta(.975, a, b))%>%
  select(-a,-b,-n,-k)

p_child_cor<- readRDS("../../stats/saves/child_all_models_cor.rds") %>%
  na.omit()%>%
  ungroup()%>%
  mutate(data_mean = rep(p_child_ex3_data%>%pull(mean),9),
         data_ci_upper = rep(p_child_ex3_data%>%pull(ci_upper),9),
         data_ci_lower = rep(p_child_ex3_data%>%pull(ci_lower),9))


ggplot(p_child_cor, aes(x = mean, y = data_mean, col = age)) +
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 1, size = .5)+
  geom_point(size = 3)+
  geom_errorbar(aes(ymin = data_ci_lower, ymax = data_ci_upper),width = 0,size = .7)+
  geom_errorbarh(aes(xmin = ci_lower, xmax = ci_upper), height = 0,size = .7)+
  coord_fixed()+
 stat_cor(method = "pearson", label.x = 0.01, label.y = 0.99, aes(x = mean, y = data_mean), inherit.aes = F, size = 3)+
  xlim(0,1)+ylim(0,1)+
  xlab("Model")+
  ylab("Data")+
  facet_grid(model~noise)+
  theme_few() + 
  scale_colour_ptol(name = "Age")+
  theme(legend.position = "bottom")



```



\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup


